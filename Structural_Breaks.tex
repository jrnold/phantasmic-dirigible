

Political and social processes are rarely, if ever, constant over time.
Thus, political scientists often have a need to estimate time-varying parameter (TVP) models.
There exist two broad approaches to estimating time-varying parameters: structural break approaches, which include dummy variables and change point models, and smoothing approaches, which include dynamic linear models and smoothing splines.
Both approaches estimate TVP when the number of observations is small relative to the number of time periods. 
Smoothing approaches keep the parameter differences small in all periods, while structural break approaches restrict parameter changes to a few locations.
These two approaches are often viewed as distinct and estimated using different methods, forcing the researcher to choose between which model to use.
Many processes the changes in the process are characterized by many periods of stability and a few periods of possibly rapid and large, change \parencite{RatkovicEng2010}.
Historically, existing smoothing methods have had a difficulty estimating these sorts of processes, so structural break models have been used \parencites{CalderiaZorn1998}{Spirling2007a}{Spirling2007b}{Park2010}{Park2011}.%
\footnote{\textcite{RatkovicEng2010} is the notable exception in that their method combines smoothly varying sections with structural breaks.}

This paper presents a simple and flexible method to estimate time-varying parameters that may be subject to structural breaks.
Time-varying parameters with possible structural breaks can be estimated within a continuous state-space model, and in particular as a dynamic linear model, by placing a shrinkage prior on the distribution of the state disturbances.
The intuition behind this can be illustrated with a simple model.
Suppose there is a vector of observed data, $y_{1}, \dots, y_{n}$, drawn from a normal distribution with a time varying mean, $\alpha_{1:n}$. 
This can be represented as a dynamic linear model,
\begin{equation}
  \label{dlm:eq:4}
  \begin{aligned}[t]
    y_{t} &= \alpha_{t} + \varepsilon_{t} & \varepsilon_{t} \sim N(0, \sigma^{2}) \\
    \alpha_{t + 1} &= \alpha_{t} + \omega_{t}
  \end{aligned}
\end{equation}
The difference between "structural breaks" and "smoothing" data-generating processes and estimation techniques is whether the $\omega$ vector is sparse (most $\omega_{t} = 0$) or dense (most $\omega_{t} \neq 0$).
A common model is to apply a normal distribution to $\omega_{t} \sim N(0, Q)$.
A normal $\omega$ can estimate change over time, but it cannot produce sparse estimates of $\omega$.
The thin tails of the normal distribution will tend to over-smooth periods around structural breaks, under-smooth periods in which there were no structural breaks.
However, estimating sparse parameter vectors is a general problem that has received considerable attention lately in large-p, small-n problems \parencites{Tibshirani1996}{PolsonScott2010}.
This paper applies some of the advances in sparse parameter estimation to estimating time-varying parameters with structural breaks.
Structural breaks, i.e. sparse $\omega$, can be estimated by placing a shrinkage prior on $\omega$.
While there are a large number of Bayesian shrinkage priors proposed, this paper will use the Horseshoe prior (HS) distribution introduced in \textcites{CarvalhoPolsonScott2009} and \textcite{CarvalhoPolsonScott2010}.
I will refer to a dynamic linear models with shrinkage priors on the state disturbances as a sparse disturbance dynamic linear models (SDDLM).

Using a sparse disturbance representation of TVP models has many favorable characteristics.
\begin{enumerate}
\item This method does not require specifying the number of structural breaks \textit{ex ante}.
The sparsity of $\omega$ will determine the number of structural breaks, and this sparsity can be estimated from the data.
Structural breaks can be detected from the posterior distribution of $\omega$ using several rules.
Not only do the number of structural breaks need not be specified beforehand, this method will work reasonable well even if the underlying data-generating process is dense, i.e. the parameter changes in each period.
\item This method is flexible.
Dynamic linear models incorporate a wide variety of models, including ARIMA and structural time-series, cubic splines, and regressions with time-varying coefficients.
Any model in which the parameter of interest can be expressed as a latent state in a dynamic linear model can be altered to assign a shrinkage prior to the state disturbance in order to make it robust to or to detect structural breaks for that parameter.
While many structural break methods are specific to single models, this paper shows how sparse-disturbance DLMs can be estimated within the general purpose Bayesian software, \Stan{}.
\item The method allows for easy estimation of structural breaks in multiple parameters which can be either independent or correlated.
Estimating independent structural breaks in a discrete state space model, like change-point models, results in the state-space multiplying exponentially.
\item Outliers can be modeled, and even included in the model, using nearly the same method, in which $\epsilon$ is assigned a shrinkage prior.
\item This method is efficient in both programmer and computational time, while still romegaining the flexibility to estimate a wide variety of models.
Since most shrinkage priors, including the HS distribution used in the paper, are scale-mixture of normal distributions, this state space model estimated is a (conditionally) Gaussian dynamic linear model (GDLM). There are can take advantage of the computationally efficient methods of mode finding and sampling from GDLMs, such as the Kalman filter and Forward-Filter Backwards-Sample.
This paper shows how a combination of \Stan{}, a general purpose Bayesian software program, and \RLang{} can be used to easily estimate and sample from the posterior of dynamic linear models.
\end{enumerate}

Section \ref{dlm:sec:struct-breaks-state} derives the sparse-disturbance DLM.
Section \ref{dlm:sec:implementation} describes a method to sample from SDDLMs using \Stan{}.
Section \ref{dlm:sec:examples} uses the SDDLMs in several example applications: Nile river flow dataset \ref{dlm:sec:nile}, George W. Bush's approval ratings \ref{dlm:sec:george-w.-bush}, and TBD.
Section \ref{dlm:sec:conclusion} concludes with discussion of how this
method can be extended.

\section{Structural Breaks within a State Space Model}
\label{dlm:sec:struct-breaks-state}

\subsection{Bayesian Shrinkage}
\label{dlm:sec:bayesian-shrinkage}

Consider a sequence of data generated from normal distributions with varying means but a common variance,%
\footnote{
  This easily extends to linear regression $y_{i} \sim N(\theta_{i} X_{i}, \sigma^{2})$ and other contexts.
}
\begin{align}
  \label{dlm:eq:13}
  y_{i} & \sim N(\theta_{i}, \sigma) & & \text{for $i \in 1:n$}
\end{align}
Furthermore, suppose that the parameter vector $\theta_{1:n}$ is sparse, meaning that most $\theta_{i} = 0$.
Define ``signals'' as those observations where $\theta_{i} \neq 0$, and ``noise'' as those observations where $\theta_{i} = 0$.
In sparse estimation, the analyst's objective is both to classify signals and noise and estimate the size of the signals.

In Bayesian estimation, there are two main classes of methods for estimating a vector of sparse parameters.
These approaches differ in the class of prior distribution of $\theta$. 
Selection methods model $\theta_{i}$ with a discrete mixture, while shrinkage methods use a continuous prior \parencite[73]{CarvalhoPolsonScott2009}. 
The discrete-mixture approach models $\theta_{i}$ with a prior consisting of a mixture between a point mass at zero and a continuous distribution for non-zero elements,
\begin{equation}
  \label{dlm:eq:1}
  \theta_{i} = w \delta_{0} +  (1 - w) g(\theta_{i}) \text{,}
\end{equation}
where $w \in [0, 1]$, $\delta_{0}$ is the degenerate distribution at 0, and $g(\theta_{i})$ is the distribution of $\theta_{i} | \theta_{i} \neq 0$.
The mixture distribution in equation \eqref{dlm:eq:1} is often called a spike-and-slab prior.
If $g$ is sufficiently heavy-tailed, then $\E(\theta_{i} | w, y_{i}) \approx w(y_{i}) y_{i}$ \parencite{PolsonScott2010}, where 
\begin{equation}
  \label{dlm:eq:7}
  w(y_{i})  = \frac{w f_{0}(y_{i})}{w f_{0}(y_{i}) + (1 - w) f_{1}(y_{i})}
\end{equation}
and $f_{0}(y) = \dnorm{y | 0, \sigma^{2}}$ and $f_{1}(y) = \int \dnorm{y | \mu, \sigma^{2}} g(\mu) \,d \mu$.
The advantage of selection methods are that they place a positive probability on the event that $\theta_{i} = 0$, and thus provides a direct test of that hypothesis.
There are both conceptual and computational disadvantages to these methods.
Conceptually, it may be implausible that $\theta_{i}$ is exactly equal to 0, especially for social science phenomena (CITE GELMAN).
Computationally, spike-and-slab priors can become intractable in high dimensions since the discrete part of the prior distribution has a support of cardinality $2^{k}$ where $k$ is the number of paramters.

Shrinkage approaches estimate sparsity using a continuous prior distribution centered at zero.
Early examples of shrinkage approaches include \parencites{Tibshirani1996}{Tipping2001}.
Shinkage priors are the Bayesian equivalent of penalized likelihood and regularized regression.
Most proposed Bayesian shrinkage priors are in the the class of global-local mixtures of normal distributions \parencite{PolsonScott2010}:
\begin{equation}
  \label{dlm:eq:3}
  \begin{aligned}[t]
    \theta_{t} &= N(0, \tau^{2} \lambda_{i}^{2}) \\ 
    \lambda_{t}^{2} &\sim p(\lambda_{t}^{2}) \\
    \tau_{t}^{2} &\sim p(\tau^{2})
  \end{aligned}
\end{equation}
A global-local normal distribution is a scale mixture of normal distributions in which the variance is the product of a global variance component $\tau^{2}$ and a local variance component $\lambda_{t}^{2}$.
The $\lambda_t$ are also called mixing parameters, and their distribution is called the mixing distribution.
Distributions in the global-local class of priors differ in the assumed distribution of the local variance component. 
Examples of commonly used shrinkage distributions and their mixing distributions include \parencite{PolsonScott2010}:%
\footnote{A normal prior, as used in ridge regression, is the trivial case in which $\lambda_{i} = 1$ for all $i$.}
\begin{description}
\item[Student's t] The mixing distribution is inverse gamma, $p(\lambda_{i}) = \dinvgamma{\xi / 2, \xi \tau^{2} / 2}$ where $\xi$ are the degrees of freedom. \parencite{Tipping2001}
\item[Double Exponential (Lasso)] The mixing distribution is exponential \parencites{ParkCasella2008}{Hans2009},
\begin{equation*}
    \begin{aligned}[t]
      p(\lambda_{i} | \tau_{i}) &= \frac{1}{2 \tau^{2}} \exp (\lambda_{i}^{2} / 2 \tau^{2}) \\
      p(\tau^{2}) &\sim \dinvgamma{\xi / 2, \xi d^{2} / 3}
    \end{aligned}
  \end{equation*}
\item[Normal-Gamma] The mixing distribution is gamma, $\lambda^{2}_{i} \sim \dgamma{a, b}$ \parencites{CaronDoucet2008}{BrownGriffin2010}.
\item[Horseshoe] The mixing distribution is inverse-beta $\lambda^{2}_{i} \sim \dinvbeta{\frac{1}{2}, \frac{1}{2}}.$ 
Equivalently, $\lambda_{i} \sim \dhalfcauchy{0, 1}$ \parencite{CarvalhoPolsonScott2010}.
\end{description}

The analog of $w$ in the global-local normal distributions is the shrinkage parameter $\kappa$.
Following standard results from a normal distribution with a normal prior on the mean and known variance, the posterior distribution of $\theta_{i}$ is
\begin{equation}
  \label{dlm:eq:11}
  \E ( \theta_{i} | y_{i}, \lambda_{i}, \tau, \sigma^{2} )  = (1 - \kappa_{i}) y_{i} \text{,}
\end{equation}
where
\begin{equation}
  \label{dlm:eq:19}
  \kappa_{i} = \frac{1}{1 + \lambda^{2}_{i} \tau^{2}} \text{.}
\end{equation}
When $\kappa_{i} \approx 0$, there is virtually no shrinkage and $\theta_{i} \approx y_{i}$.
However, when $\kappa_{i} \approx 1$, there is almost complete shrinkage and $\theta_{i} \approx 0$.
Note that both $\lambda$ and $\tau$ contribute to the shrinkage parameter.
When $\tau$ is small, then most values of $\kappa_{i} \approx 1$ and $\theta$ is sparse.
However, for any given observation, if $\lambda_{i}^{2}$ is sufficiently large, it can overwhelm the effect of $\tau$, and $\kappa_{i}$ can be close to 0, leaving $\theta_{i}$ unshrunk.
Thus, the global variance component $\tau$ determines the overall sparseness of $\theta_{1:n}$, while the local variance components $\lambda_{i}^{2}$ determine the non-zero values of $\theta_{i}$.

Note that different different distributions of $\lambda_{i}$ imply different distributions of the shrinkage prior $\kappa_{i}$.
The implied distribution of the shrinkage prior provides an intuitive way to understand the shrinkage behavior of prior distributions.
For example, the horseshoe distribution is so-called because it implies a U-shaped distribution over the the shrinkage parameter, $p(\kappa_{i}) = \dbeta{\frac{1}{2}, \frac{1}{2}}$.

While there are a large number of proposed shrinkage priors \parencites{ArmaganDunsonLee2011}{BrownGriffin2010}{PolsonScott2010}, this paper will use the Horseshoe distribution \parencites{CarvalhoPolsonScott2009}{CarvalhoPolsonScott2010}{PolsonScott2010}{PolsonScott2012}{DattaGhosh2012}.

A ``good'' shrinkage prior should have two properties: tail-robustness and super-efficiency.
The horseshoe prior distribution is one of the few shrinkage priors that possess these two quantities.
It may not be the optimal distribution, and this is an area which is rapidly developing.
However, it seems to be a good ``default'' shrinkage prior that will likely work well in most circumstances.

While the Horseshoe distribution has no analytical form, it can be represented as a scale-mixture of normal distributions (equation \eqref{dlm:eq:3}( in which the local scale components $\lambda$ are distributed half-Cauchy,%
\footnote{
The half-Cauchy prior on the scale components $\lambda_{t}$ implies an inverse-Bomega (Bomega prime) distribution on the variance components $\lambda_{t}^{2} = \dinvbeta{\frac{1}{2}, \frac{1}{2}}$, where $\dinvbeta{x; a, b} = \frac{x^{a - 1} (1 + x)^{-a - b}}{B(a, b)}$, where $B$ is the Beta function.
The inverse beta distribution is related to the Beta distribution.
If $X \sim \dbeta{a, b}$, then $\frac{1}{1 + X} \sim \dinvbeta{a, b}$.
}
\begin{equation}
  \label{dlm:eq:6}
  \lambda_{t} \sim \dhalfcauchy{0, 1}
\end{equation}
The Horseshoe prior distribution has many appealing properties as a shrinkage prior distribution.
Intuitively, these follow from its heavy Cauchy-like tails which allow large signals to remain unshrunk, and its infinitely high spike at zero, which aggressively shrinks noise.
\footnote{See \textcites{CarvalhoPolsonScott2010}{CarvalhoPolsonScott2009}{DattaGhosh2012} for formal properties of the horseshoe prior.}
Figure \ref{dlm:fig:horseshoe} compares the density function of the Horseshoe Prior to the normal, Cauchy, and Laplacian (double-exponential) distributions, both near zero and in the tail.
\begin{figure}
  \centering
  % \includegraphics{file.path(fig_path, "fig-horseshoe1")}
  % \includegraphics{file.path(fig_path, "fig-horseshoe2")}
  \caption{The density of the horseshoe prior distribution (in black) compared with the densities of the normal, Cauchy, and Laplacian distributions (in gray).}
  \label{dlm:fig:horseshoe}
\end{figure}

\subsection{Local Level Model}
\label{dlm:sec:local-level-model}

In equation \ref{dlm:eq:13}, the means for each observation are independent, but drawn from a common distribution.
In the time-varying parameter case, the observations are ordered and the mean of each observation depends on the mean of the previous observation.
The simplest case of this is the local-level model in which the parameters evolve as a random walk,
\footnote{Also called the \textit{1st-order polynomial trend model} \parencite[Chapter 2]{WestHarrison1997} or random walk with noise.}
\begin{equation}
  \label{dlm:eq:5}
  \begin{aligned}[t]
    y_{t} &= \mu_{t} + \nu_{t} & \nu_{t} & \sim \dnorm{0, V} \\
    \mu_{t + 1} &= \mu_{t} + \omega_{t} & \omega_{t} & \sim \dnorm{0, W_{t}} \\
    \theta & \sim N(m_{0}, C_{0})
  \end{aligned}
\end{equation}

Since all the component equations in equation \eqref{dlm:eq:5} are (multivariate) normal, the joint distribution of $\theta$ and $y$ is also multivariate normal.
For each time period $t$, estimate of $\theta_{t}$ can be updated using the following equations.
The prior distribution of the state $\theta_{t}$, given observations $y_{1:(t-1)}$, is
\begin{equation}
  \label{dlm:eq:23}
  \begin{aligned}[t]
    \theta_{t} | y_{1:(t-1)} & \sim N(m_{t-1}, R_{t}) \\
    R_{T} & = C_{t - 1} + W
  \end{aligned}
\end{equation}
The predictive distribution of the data $y_{t}$, given observations $y_{1:(t-1)}$, is
\begin{equation}
  \label{dlm:eq:25}
  \begin{aligned}[t]
    y_{t} | y_{1:(t-1)}  \sim N(f_{t-1}, Q_{t}) \\
    f_{t-1} & m_{t-1} \\
    Q_{t} & C_{t - 1} + W + V
  \end{aligned}
\end{equation}
The posterior distribution $\theta_{t}$ at time $t$, given observations $y_{1:t}$, is
\begin{equation}
  \label{dlm:eq:26}
  \begin{aligned}
    \theta_{t} | y_{1:t} \sim N(m_{t} = m_{t-1} + A_{t} e_{t}, C_{t} = A_{t} V)    
  \end{aligned}
\end{equation}
where $A_{t} \in [0, 1]$ is the adaptation coefficient and $e_{t}$ is the forecast error, defined as
\begin{align}
  \label{dlm:eq:30}
  A_{t} = R_{t} / Q_{t} = \frac{C_{t-1} + W_{t}}{C_{t-1} + W + V} \\
  e_{t} = y_{t} - f_{t-1} = y_{t} - m_{t-1}
\end{align}
Together equations \eqref{dlm:eq:23}-\eqref{dlm:eq:26} are known as the \textit{filtering equations} or the Kalman filter.
After filtering, the values of $p(\theta_{t} | y_{1:T})$ can be sampled by forward filtering backward sample.
However, only the filtering equations are needed to calculate the likelihood of equation \eqref{dlm:eq:5},
\begin{equation}
  \label{dlm:eq:27}
  p(y_{1:t} | V, W_{t}) \propto -\frac{1}{2} \sum_{i = 1}^{t}  \left( \log | Q_{t} |  + e_{t}^{2} / Q_{t} \right)
\end{equation}

An important feature of the filtering equations is that the filtered estimate of the mean, $m_{t} = E p(\theta_{t} | y_{t})$, is a weighted sum of the mean of the prior,  $m_{t - 1}$, and the forecast error $e_{t} = y_{t} - m_{t - 1}$.
Equation \eqref{dlm:eq:26} can also be rewritten as a weighted sum of the new observation $y_{t}$ and the prior mean $m_{t-1}$,
\begin{equation}
  \label{dlm:eq:18}
  \theta_{t} | y_{1:t} = (1 - A_{t}) m_{t-1} + A_{t} y_{t}
\end{equation}
The adaptation coefficient is the weight assigned to the new observation and is a function of the observation and system variances,
\begin{equation}
  \label{dlm:eq:24}
  A_{t} = R_{t} / Q_{t} = \frac{C_{t-1} + W_{t}}{C_{t-1} + W + V}
\end{equation}


In this case, the estimates of $\theta_{t}$ are not shrunk towards 0, but are shrunk towards the previous period's value, $\theta_{t-1}$.
Alternatively, in this case, the shrinkage is not over the values of $\theta_{t}$, but the values of the one period differences $\theta_{t} - \theta_{t -1}$.
A model with complete shrinkage is the constant mean model in which $\theta_{t} = m_{0}$ for all $t$, which occurs when $A_{t} = 0$ for all $t$.
A sparse model is one in which most $A_{t} \approx 0$, but a few $A_{t}$ can be far from 0.
Ideally, a prior distribution could be placed on $A_{1:t}$ that encourages sparsity, both pushing values towards 0 and 1, and regulating the number of sparse parameters.

Sparsity in this case is over $omega$, not $\theta$.

\subsection{Dynamic Linear Models}
\label{dlm:sec:dynam-line-models}

The local level model in the previous section is a special case of a wider class of time-series models known as Gaussian Dynamic Linear Models (DLMs).%
\footnote{Gaussian DLMs are also called normal DLMs or Gaussian linear state space models.}
With minor alterations, the shrinkage methods developed in the previous section can generalize to DLMs.
Many common models, including SARIMA, structural time series models with level, trend, cyclical and seasonal components, regressions with time-varying coefficients, cubic splines, and stochastic volatility models can be represented as DLMs. 
\textcites{WestHarrison1997}{DurbinKoopman2001}{CommandeurKoopman2007}{PetrisPetroneEtAl2009}{ShumwayStoffer2010} provide thorough treatments of state space models and their applications.

A Gaussian dynamic linear model (DLM) for a $n$-dimensional observation sequence $y_{1}, \dots, y_{n}$ is defined by the following set of equations.%
For $t = 1:n$,
\begin{align}
  \label{dlm:eq:8}
  \underset{r \times 1}{y_t} &= \underset{n \times r}{F_{t}}' \, \underset{n \times 1}{\theta_t} + \underset{r \times 1}{\nu_t} & \nu_{t} &\sim \dmvnorm{0, V_{t}}{q} \\
  \label{dlm:eq:14}
  \underset{n \times 1}{\theta_{t+1}} &= \underset{n \times n}{G_{t}} \, \underset{n \times 1}{\theta_{t}} + \underset{n \times 1}{\omega_{t}} & \omega_{t} &\sim \dmvnorm{0, W_{t}}{n} \\
  \label{dlm:eq:2}
  \theta_{1} & \sim \dmvnorm{m_{0}, C_{0}}{n}
\end{align}
Equation \eqref{dlm:eq:8} is the \textit{observation equation} which relates the \textit{observation vector} $y_{t}$ to the \textit{state vector} $\theta_{t}$.
Equation \eqref{dlm:eq:14} is the \textit{state equation} which describes the evolution of the states over time.
Equation \eqref{dlm:eq:2} is the \textit{initial state equation}.
The vectors $\nu_{t}$ and $\omega_{t}$ are referred to as the \textit{observation} and \textit{state disturbances}, respectively.
The matrices $F_{t}$, $G_{t}$, $V_{t}$, and $W_{t}$ are referred to as the \textit{system matrices}.
Let $\mathcal{S}_{t}$ refer to the set of system matrices.
The matrix $F_{t}$ is the design matrix, $G_{t}$ is the transition matrix, $V_{t}$ is the observation covariance matrix, and $W_{t}$ the state covariance matrix.

The range of models which can be represented as DLMs means that the methods presented below apply to a wide range of models.
Any model in which the parameter of interest can be represented as a state in a DLM, can be estimated using the following method to detect structural breaks.
%\todo[inline]{Should I be talking about ``detecting'' structural  breaks or dealing with large changes (heavy tailed distributions)?}

As in the local level model, since all equations in \eqref{dlm:eq:8} through \eqref{dlm:eq:14} are multivariate normal, they can be updated sequentially with conjugate forms.%
\footnote{See \textcite[Section 4.3][]{WestHarrison1997} for proofs.}
The prior for the state vector at time $t$ is $\theta_{t} \sim N(a_{t}, R_{t})$ where
\begin{align}
  \label{dlm:eq:29}
  \theta_{t} | y_{t} &= 
  a_{t} &= G_{t} m_{t-1} \\
  R_{t} &= G_{t} C_{t-1} G'_{t-1} + W_{t}
\end{align}
At $t-1$ the one step ahead predictive distribution of $y_{t}$ is,
\begin{align}
  \label{dlm:eq:28}
  y_{t} | \theta_{t} & \sim N(f_{t}, Q_{t}) \\
  Q_{t} &= 
\end{align}
At $t$ the posterior distribution of $\theta$ is,
\begin{align}
  \label{dlm:eq:15}
  \theta_{t} | y_{t} \sim N(m_{t}, C_{t}) \\
  m_{t} = a_{t} + A_{t} e_{t} \\
  C_{t} = R_{t} - A_{t} Q_{t} A_{t}' \\
  A_{t} = R_{t} F_{t} Q^{-1}_{t} \\
  e_{t} = y_{t} - f_{t}
\end{align}
Like the local level case, the filtered posterior mean of the state,
$m_{t}$, is a weighted combination of the predictive distribution's
mean $a_{t}$ (which in this case is not equal to $m_{t-1}$) and the
forecast error, $e_{t}$.
And in the general case, the adaptive coefficient, $A_{t}$, is an $n
\times r$  matrix.
Thus the weight given a current data point $y_{t}$ depends on the
observation variance $V_{t}$ through $Q_{t}$, and the prior variance
$C_{t-1}$ and state covariance matrix $W_{t}$ through $R_{t}$.


The previous section applied sparse disturbance for the local level model, but the use of shrinkage priors on the state disturbances can easily be extended to the more general DLM in section \ref{dlm:sec:dynam-line-models}.
First, consider the case in which the state disturbances are uncorrelated.
\footnote{
  For simplicity, assume that $m = r$ and $R_{t} = I_{m}$.
  In general, $R_{t}$ are sparse selection matrices.
}
\begin{equation}
  \label{dlm:eq:20}
  \begin{aligned}[t]
    \omega_{t,i} &\sim \dnorm{0, \lambda_{t,i}^{2} \tau_{i}} \\
    \lambda_{t,i} & \sim \dhalfcauchy{0, 1}
  \end{aligned}
\end{equation}
where $\lambda$ is a $n \times r$ matrix, and $\tau$ is a $r \times 1$ vector.

Next, consider the case in which the state disturbances, $\omega_{t}$ are correlated.
In that case, the state disturbances are distributed as a scale mixture of multivariate normal distributions.
The multivariate equivalent of equation \eqref{dlm:eq:3} is,
%\todo{This derivation of the scale mixture of MVN seems intuitive, but I haven't seen it done}
\begin{equation}
  \begin{aligned}[t]
    \omega_{t} &\sim \dmvnorm{0, \Lambda_{t} \Gamma \Lambda_{t}'}{m} \\
    \Lambda \Lambda' & \sim p(\Lambda \Lambda')
  \end{aligned}
\end{equation}
where $\Gamma$ is the global covariance component, and $\Lambda_{t}$ is a lower triangular matrix such that  $\Lambda_{t} \Lambda_{t}'$ is the local covariance matrix.
The state disturbances $\omega_{t}$ can be correlated either at the global or local scale.
$\Gamma$ determines the scale and correlation of the global shrinkage parameters.
A non-diagonal $\Gamma$ implies that the sparsity of the state disturbances is correlated.
$\Lambda$ controls the scale and correlation of the local shrinkage parameters.
A non-diagonal $\Lambda$ implies that the sizes of individual disturbances (structural breaks) is correlated.

To generalize the horseshoe prior distribution to matrix variate random variables, decompose $\Lambda_{t} \Lambda_{t}'$ into a standard deviation vector ($\lambda_{t}$) and a correlation matrix ($R_{t}$),%
\begin{align}
  \label{dlm:eq:16}
  \Lambda_{t} \Lambda_{t}' &= \diag(\lambda_{t}) R_{t} \diag(\lambda_{t})' \\
  \label{dlm:eq:17}
  \lambda_{t,i} &= \dhalfcauchy{0, 1} & \text{for $i \in 1:q$}
\end{align}


\subsection{Classifying Structural Breaks}
\label{dlm:sec:structural-breaks}

Given posterior estimates from a sparse disturbance dynamic linear model, there are a few methods that can be used to detect structural breaks.

The first method uses the posterior distribution of $\omega_{t}$ to detect structural breaks in the corresponding $\alpha$.
A structural break can be classified as an $\omega_{t}$ in which which the (95\%) credible interval of the posterior distribution excludes zero.
This is the Bayesian equivalent to the auxiliary residual test of \textcites{JongPenzer1998}{DurbinKoopman2001}.%
In addition to the use of a credible interval rather than a confidence interval, the Bayesian method differs from the auxiliary residual test in that it integrates over the posterior distribution of the state matrices $S_{t}$ rather fixing them at point estimates.
Note that in some applications, especially those in which the transition matrix $T_{t}$ includes parameters, it may make more sense to use the posterior distribution of the difference in the states, $p(\Delta \alpha_{t - 1} | y)$.

The second method uses the local variance components $\lambda_{t}$ to identify structural breaks \parencite[179-180]{PetrisPetroneEtAl2009}.
If $\lambda_{t} = 1$, then the state disturbance is distributed normal with a scale equal to the global scale $\omega_{t} \sim N(0, \tau^{2})$.
Thus, the local shrinkage parameters $\lambda$ are relative measure of how much each $\omega_{t}$ is shrunk towards zero.
Values of $\lambda_{t} > 1$ ($\lambda_{t} < 1$) indicate state disturbances that are dispersed (shrunk) relative to the global scale.
A structural break can be classified as a $t$ in which $\E (p(\lambda_{t} | y, .)) > 1$.
Alternatively, the probability of a structural break is $\Pr(p(\lambda_{t} | y, .) > 1)$.

The examples in section \ref{dlm:sec:examples} will use both of these methods.

\subsection{Outliers}
\label{dlm:sec:outliers}

Most of the discussion on modeling structural breaks directly applies to modeling outliers.
Outliers can be modeled with a DLM by placing a shrinkage prior on the observation disturbance vector $\nu$. 
The discussion in the previous sections applies to outliers if references to $\omega$ are replaced with $\nu$.
A difference difference between observation disturbances and state disturbances, is that there is usually a higher prior belief of sparsity in state disturbances than there is in observation disturbances.
Almost always observation disturbances are expected to be non-zero;
there are many statistical models in which the parameters are time-invariant, but none in without observation errors.%
Outliers are not the few periods in which $\nu_{t} \neq 0$, but are instead the few periods with observation disturbances of very large magnitudes, $|\nu_{t}| \gg 0$.
In this case, it is more important for the shrinkage prior to have heavy tails than have a spike at zero. 
For that reason, the $t$-distribution, which is commonly used to model
outliers in Bayesian regression and DLMs \parencite{West1984}, is
likely sufficient for most applications.

\section{Implementation}
\label{dlm:sec:implementation}

MCMC sampling from a dynamic linear model is challenging due to the temporal dependence of the latent state parameters $\alpha$ \parencite{ReisSalazarGamerman2006}.
Although full conditional distributions for a Gibbs sampler can be easily specified \parencite{CarlinGelfandSmith1992}, in practice sampling each $\alpha_{t}$ will almost always result in slow convergence and highly correlated posterior samples.
However, for Gaussian dynamic linear models there exist more efficient block sampling algorithms which sample $\alpha_{1:n}$ as a single block, improving the the speed of convergence and the number of effective samples.%
\footnote{See \textcite{ReisSalazarGamerman2006} for a comparison of the efficiency of various sampling methods, and \textcite{migon2005dynamic} for an overview of the various sampling methods.}
These block methods include the Forward-Filter Backward Smoothing algorithm of \textcite{CarterKohn1994} and \textcite{Fruehwirth-Schnatter1994}, as well as more efficient simulation smoothers of \textcite{DeJongShephard1995}, \textcite{DurbinKoopman2002}, \textcite{StricklandTurnerDenhamEtAl2009}%
An alternative approach to block sampling, is to sample directly from the posterior $p(\alpha | y, S)$ which for GDLMs is a multivariate normal distribution with a sparse, block diagonal covariance matrix, samples can be drawn directly from the posterior distribution \parencites{migon2005dynamic}{ChanJeliazkov2009}.
However, none of these more efficient algorithms are included in the commonly used general-purpose Bayesian software programs (\proglang{BUGS}, \proglang{JAGS}, and PyMc).
Thus, for savings in computational time a researcher needs to trade-off programmer time to implement a custom MCMC sampler that uses one of the block sampling methods.
While there exists many software implementations of filters and smoothers for Dynamic Linear Models,%
\footnote{Reviewed in a special issue of the \textit{Journal of Statistical Software} \textcite{CommandeurKoopmanOoms2011} and \textcite{Tusell2011}.}
writing an efficient sampler may there are also require additional work in order to reduce to the correlation between the states and parameters in the system matrices \parencites{ReisSalazarGamerman2006}{ChibNardariShephard2006}.
This implementation cost reduces the ease of using DLM models in applications despite their flexibility and intuitive advantages over traditional time series methods \textcite[51]{DurbinKoopman2001}.

This paper proposes and implements an easy and practical method to estimate DLMs within \Stan{} \parencite{Stan2013}.
This method is used within this paper for SDDLMs but can be applied to any DLM.
I divide the problem of sampling $p(\alpha, S_{t} | y_{t})$ into two steps.
First, sample from $p(S_{t} | y_{t})$. 
Second, given samples from $p(S_{t} | y_{t})$, sample the states $p(\alpha_{t} | S_{t}, y_{t})$.

First, sample from $p(S_{t} | y_{t})$ using \Stan{}'s implementation of HMC.
\Stan{} is a general purpose Bayesian software program \parencite{Stan2013b}.
Like BUGS, it has a domain specific language that lets the user specify the statistical model without specifying the steps used to sample from the model.
Unlike BUGS, \Stan{} is not based on Gibbs sampling, but instead uses a variant of Hamiltonian Monte Carlo (HMC) called NUTS \parencite{HoffmanGelman2013}.
While as of the time of the writing, a distribution for dynamic linear models is not included in the software, but a method for efficiently sampling the other parameters while marginalizing over the latent states can be implemented within the \Stan{} modeling language.
In particular, in order to implement a custom distribution only its log-likelihood need to be calculated \parencite[Chapter 17]{Stan2013}
The log-likelihood $p(y_{t} | S_{t})$ of a DLM can be easily and efficiently calculated with a single pass through the data using the Kalman Filter.%
\footnote{Derivations of the Kalman Filter can be found in any state space textbooks; e.g. \textcite{DurbinKoopman2001}.}
These Kalman Filter and log-likelihood calculations are written in \Stan{}'s modeling language;
\ref{dlm:sec:code} has an example which estimates a local level model with state disturbances distributed horseshoe (this is the model used in \Model{Nile}{hs} in Section \ref{dlm:sec:nile}).
\footnote{
  I originally implemented the DLMs in \Stan{} by directly transcribing the observation and system equations of the DLM.
  This works reasonably well, and \Stan{} achieved better posterior mixing than JAGS.
  However even using optimizations for \Stan{} models, I encountered two problems with that approach.
  First, it approach scaled poorly, and seemed to slow down dramatically as $n$ and $T$ increased.
  Second, although in most periods the chains would mix well, they could get stuck in certain regions of the posterior regions for long sequences.
  The exact reason for this is unknown, but this is likely due to the geometry of the posterior distribution and the inability of HMC with Euclidean geometry to adapt to different scales in different parts of the parameter space.
  When \Stan{} implements Riemann Manifold HMC, it may solve this problem.
}

Second, having obtained a sample of size $K$ from $p(S | y)$, for each $k \in 1:K$ sample from $p(\alpha | y, S^{(k)})$ using a block sampler of the type discussed earlier.
For this paper, I used the \RLang{} package \pkg{KFAS} \parencite{Helske2012} which has an implementation the \textcite{DurbinKoopman2002} simulation smoother algorithm.
In practice, the most difficult aspect of this step is converting the output from the first step into the data structures needed to draw samples in the second step.
For this purpose, and in general to make post-processing MCMC samples easier, I wrote and have made available the \RLang{} package \href{https://github.com/jrnold/mcmcdb}{\textbf{mcmcdb}}.
Separating the sampling of $p(\alpha_{t} | y, S)$ from $p(S | y)$ has a few practical advantages.
In some cases, e.g. many ARIMA models, only parameters in the system matrices are of interest, and thus the step $p(\alpha | y, S)$ can be skipped.
Second, once a sample from $p(S | y)$ is obtained, the sampling of $p(\alpha | y, S)$ can be done in parallel.


\section{Examples}
\label{dlm:sec:examples}

\subsection{Nile River Flow}
\label{dlm:sec:nile}

The Nile data is a series of readings of the annual flow volume of the Nile River at Aswan taken between 1871 and 1970.
This dataset is a classic dataset that has been analyzed in many works on structural breaks and DLMs \parencites{Cobb1978}{Balke1993}{JongPenzer1998}{DurbinKoopman2001}{CommandeurKoopmanOoms2011}.%
\footnote{The dataset is included with base \RLang{} as \texttt{datasets::Nile} \parencite{RCT2013}.}
Previous analyses show a single level shift in the data, with the shift occurring in 1899.
This level shift is attributed to the construction of a dam at Aswan in that year.
%\todo{What is this}

For the Nile River model, I will estimate and compare a SDDLM with two alternative models; one with a normal distribution on the state disturbances and one with a normal distribution and a manual intervention on the state disturbances.
All of these models will be variants of the following local level model,
\begin{equation}
  \label{dlm:eq:21}
  \begin{aligned}[t]
    y_{t} &\sim \dnorm{\alpha_{t}, H_{t}} \\
    \alpha_{t + 1} &\sim \alpha_{t} + \omega_{t} \\
    \alpha_{1} &\sim N(y_{1}, \Var y_{1:n})
  \end{aligned}
\end{equation}
The models only differ in the distribution of $\omega_{t}$.

In the first model, \Model{nile}{hp}, $\omega_{t}$ has a horseshoe prior,
\begin{equation}
  \label{dlm:eq:22}
  \begin{aligned}[t]
    \omega_{t} & \sim \dnorm{0, \lambda^{2}_{t} \tau_{t}^{2}} & \lambda_{t} & \sim \dhalfcauchy{0, 1}
  \end{aligned}
\end{equation}
In the second model, \Model{nile}{normal}, $\omega$ is  drawn i.i.d. from a normal distribution as in \textcite{DurbinKoopman2001} and \textcite{petris2011state},
\begin{equation}
  \label{dlm:eq:9}
  \begin{aligned}[t]
    \omega_{t} & \sim \dnorm{0, \tau_{t}^{2}}
  \end{aligned}
\end{equation}
The third model, \Model{nile}{inter}, adds state intervention to \Model{nile}{inter} to account for a structural break in 1899.
If the locations of structural breaks are known, then they can be modeled within the DLM by increasing the variance of the state disturbance at those periods \textcite[Chapter 11][]{WestHarrison1997}.
By increasing the variance of $\omega_{t}$, the estimated state discounts previous observations and is able to adjust more quickly to the data in new observations.
This is the same mechanism that SDDLMs use to model structural breaks, but whereas manual interventions increase the variance to periods based on prior information, SDDLMs increase the variance of periods based on model fit.
The intervention is modeled as follows,
\begin{equation}
  \label{dlm:eq:12}
  \begin{aligned}[t]
    \omega_{t} & \sim \dnorm{0, \tau_{t}^{2} + \omega_{t}^{2}} \\
    \omega_{t} & = 
    \begin{cases}
      10^{6} & \text{if $t = 28$} \\
      0 & \text{otherwise}
    \end{cases}
  \end{aligned}
\end{equation}
%\todo[inline]{Change to multiplicative intervention so it is exactly analogous to the SDDLM.}

Figure \ref{dlm:fig:nile-posterior} plots the data and the the posterior distributions of the level ($p(\alpha | y)$) for all three models.
\Model{nile}{hs} and \Model{nile}{inter} clearly show a structural break at 1899.
\Model{nile}{normal} smooths over the break, showing a downward trend several years before and after 1899.
\Model{nile}{normal} also shows much more variation in the posterior distribution before and after the break, whereas both \Model{nile}{hs} and \Model{nile}{inter} show (almost) no changes in level apart from 1899.
In other words, the normal model over-smooths at the break, and under-smooths in other periods.
This is a general feature of using a normal prior because it only has a global variance component and no local variance components.
Because it only has a single parameter to account for variation, it must trade off shrinking noise and non-shrinking signals.
To account for the large disturbance in 1899, the estimated $\tau$ must increase, but this larger $\tau$ will leave the other periods relatively un-shrunk.
On the other hand, a global-local scale mixture like the horseshoe prior has two parameters which can handle
The global variance component $\tau^{2}$ shrinking the noise and the determines the overall sparsity of the parameter vector.
The local variance components $\lambda^{2}_{t}$ are able to let individual parameters remain unshrunk if $\lambda_{t}^{2}$ is estimated to be large enough.

Table \ref{dlm:tab:nile-fits} displays model fit statistics for the Nile River flow models.
The primary criterion that is used to compare the models is the Widely Applicable Information Criterion (WAIC) \parencite{Watanabe2010}.%
\footnote{See \textcite{GelmanHwangVehtari2014a} and \textcite{VehtariOjanen2012} for recent overviews of Bayesian model comparison.}
The WAIC is an information criterion similar to the commonly used Deviance Information Criterion (DIC) \parencite{spiegelhalter2002bayes}, but unlike the DIC, the WAIC is applicable to singular models such as mixture models.
Fundamentally, the WAIC is a measure of expected out-of-sample prediction error, and is asymptotically equivalent to Bayesian leave-one-out cross-validation \parencite{Watanabe2010}.
%\todo[inline]{Add footnote with WAIC derivation}
Like the DIC, WAIC approximates the expected log predictive density of new data without cross-validation by using the within-sample predictive accuracy adjusted by a bias term which corrects for the effective number of parameters.
Models with lower WAIC have better fits.
Table \ref{dlm:tab:nile-fits} shows that \Model{nile}{hs} has the lowest WAIC, and thus the best fit.
However, \Model{nile}{hs} has a higher RMSE and $\chi^{2}$ deviance than \Model{nile}{normal} and \Model{nile}{inter}.

Diagnostics to classify structural breaks are plotted in Figures \ref{dlm:fig:nile-omega} and \ref{dlm:fig:nile-lambda}.
Figure \ref{dlm:fig:nile-omega} displays summaries of the posterior distributions of the the state disturbances $p(\omega_{t} | y)$.
Structural breaks are classified as those periods in which the 95\% credible interval of $p(\omega_{t} | y)$ excludes zero.
\Model{nile}{hs} and \Model{nile}{inter} both classify 1899 as the only structural break in the level, while \Model{nile}{normal} classifies no periods as structural breaks since all the credible intervals cross zero.
However, in \Model{nile}{hs} the credible intervals of $p(\omega|y)$ are much tighter than those of \Model{nile}{normal}.
Figure \ref{dlm:fig:nile-lambda} displays the values of the median of the local variance components $p(\lambda_{t} | y)$ in \Model{nile}{normal}.
Structural breaks are classified as observations for which the median is greater than 1, and as expected and should be the case, only 1899 is classified as a structural break.

This example shows that using the horseshoe prior on the state disturbances is able to recover the one known structural break for the Nile dataset.
While this may seem like an easy application, it is an example with a sparse disturbance vector (expected 1 of 100), the domain in which continuous state space models have traditionally captured time-varying-parameters poorly (e.g. see \Model{nile}{normal}.).
However, this example shows that a scale mixture of normal distributions can both recover a structural break in a series while shrinking changes in the other periods to zero.

\begin{figure}[htpb]
  \centering
  \begin{subfigure}{1.0\textwidth}
    %\includegraphics{file.path(fig_path, "fig-nile1")}
    \caption{\Model{nile}{hs}}
    \label{dlm:fig:nile-posterior-1}    
  \end{subfigure}
  \begin{subfigure}{1.0\textwidth}
    %\includegraphics{file.path(fig_path, "fig-nile2")}
    \caption{\Model{nile}{normal3}}
    \label{dlm:fig:nile-posterior-2}
  \end{subfigure}
  \begin{subfigure}{1.0\textwidth}
    %\includegraphics{file.path(fig_path, "fig-nile3")}
    \caption{\Model{nile}{inter}}
    \label{dlm:fig:nile-posterior-3}
  \end{subfigure}
  \caption{Plots of the posterior distributions of the level, $p(\alpha | y)$, for Nile river flow models. Points are the data; the line $E p(\alpha | y)$; the ribbon is the 95\% credible interval of $p(\alpha | y)$. A dam at Ashwan was built in 1899}
  \label{dlm:fig:nile-posterior}
\end{figure}

\begin{table}[htpb]
  \centering
  % \input{file.path(tab_path, "tab-nile-fits.tex")}
  \caption{Model fit comparison of Nile river flow models}
  \label{dlm:tab:nile-fits}
\end{table}

\begin{figure}[htpb]
  \centering
  %\includegraphics{file.path(fig_path, "fig-nile-omega")}
  \caption{Plot of $p(\omega | y)$ from each model of Nile flows.}
  \label{dlm:fig:nile-omega}
\end{figure}

\begin{figure}[htpb]
  \centering
  %\includegraphics{file.path(fig_path, "fig-nile-lambda")}
  \caption{Plot of $\Median p(\lambda | y)$ from \Model{nile}{hs}}
  \label{dlm:fig:nile-lambda}
\end{figure}

\clearpage{}

\subsection{George W. Bush Approval Ratings}
\label{dlm:sec:george-w.-bush}

The motivating example of \textcite{RatkovicEng2010}.

The data are approval ratings for George W. Bush during his terms as president.
The consist of 270 observations between January 24, 2001 and December 19, 2008.%
The polling data come from multiple houses and are the polls listed on the \href{http://webapps.ropercenter.uconn.edu/CFIDE/roper/presidential/webroot/presidential_rating_domegail.cfm?allRate=True&presidentName=Bush#.UbeB8HUbyv8}{Roper Center Public Opinion Archives}.

This set of data pose a challenge for estimation using traditional smoothing methods due to what \textcite{RatkovicEng2010} call, perhaps hyperbolically, the ``9/11 problem'' in the data.
If anything, Bush's approval was tending downward up to September 9, 2001.
However, in an effort to incorporate the approximately 35 percentage point increase in approval after September 11, 2001 will tend to show an uptick in his approval well before 9/11.

I estimate two models on this data. 
Both models are local level models which only differ in the distribution of $\omega$.
In \Model{bush}{hs} $\omega$ is distributed horseshoe, and in \Model{bush}{normal} $\omega$ is distributed normal.
Figure \ref{dlm:fig:bush-posterior} plots the data and the posterior distributions of $\alpha$ for both models.
In \Model{bush}{hs}, the approval rating for Bush does not increase until 9/11, while \Model{bush}{normal}, the approval rating is increasing from approximately June.
In \Model{bush}{hs}, the approval rating for Bush does not increase until 9/11, while \Model{bush}{normal}, the approval rating is increasing from approximately June.

Table \ref{dlm:tab:bush-fits} tabulates model fit statistics for the Bush approval data models. 
The results are similar to that for the Nile river data; the horseshoe prior model has a lower WAIC, lower log-likelihood, and lower effective number of parameters, but a higher RMSE than other models.

%\textit{TODO: discuss the structural breaks.}

\textit{It may be better to estimate this model with local trends, and estimate structural breaks in both levels and trends.}

\begin{table}[htpb]
  \centering
  % \input{file.path(tab_path, "tab-bush-fits.tex")}
  \caption{Model fit statistics for Bush approval rating models.}
  \label{dlm:tab:bush-fits}
\end{table}

\begin{figure}[htpb]
  \centering
  \begin{subfigure}{1.0\textwidth}
    %\includegraphics{file.path(fig_path, "fig-bush1")}
    \caption{\Model{bush}{hs}}
    \label{dlm:fig:bush1}
  \end{subfigure}
  \begin{subfigure}{1.0\textwidth}
    %\includegraphics{file.path(fig_path, "fig-bush2")}
    \caption{\Model{bush}{normal}}
    \label{dlm:fig:bush2}
  \end{subfigure}
  \caption{Plots of the posterior distributions of the level, $p(\alpha | y)$, for Nile river flow models. Points are the data; the line $E p(\alpha | y)$; the ribbon is the 95\% credible interval of $p(\alpha | y)$.}
  \label{dlm:fig:bush-posterior}
\end{figure}


\subsection{Greenbacks}
\label{dlm:sec:greenbacks}

The paper \textcite{WillardGuinnaneEtAl1996} finds turning points in the prices of Union currency against gold during the American Civil War using the multiple structural break method of Perron et al (2003).

\textit{I've only run a few preliminary models with this so far, but the results do not show clean structural breaks like the Bush and Nile examples. It looks more like a simple smoothing.}


\section{Conclusion}
\label{dlm:sec:conclusion}

This paper shows that a simple tweak to dynamic linear models allows them to estimate structural break models with a random number of breaks.
The sparse disturbance approach is both intuitive and flexible, while remaining computationally efficient.

\begin{enumerate}
\item Write a DLM distribution for \Stan{}.
\item Write a new R package for Dynamic Linear Models. There are a few reasons for this, despite there being multiple R packages that implement Kalman Filter / DLM capabilities. 
The existing packages do not implement direct sampling from the posterior distribution. 
There is also a technical and clarity advantage to rewriting a Kalman filter library with Rcpp backend. In fact, the primary RcppArmadillo example is 
a Kalman filter.
\end{enumerate}

\clearpage{}

\section{Appendix}
\label{sec:appendix
}
\subsection{code}
\label{dlm:sec:code}

This is an example \Stan{} model for a local level DLM with Horseshoe Prior distributions on the state disturbances; this is the \Stan{} model used to estimate \Model{nile}{hp} in Section \ref{dlm:sec:nile}. 
The Kalman filter section of the \Stan{} code is specialized to the local level model case.

\begin{singlespace}
%  \include{code-local_level_hs}  
\end{singlespace}

\subsection{Sparse Discount Filter}
\label{dlm:sec:sparse-disc-filt}

An alternative approach to sparse disturbances in a DLM is to directly set a prior on the the discount factors, $\delta_{1:T}$. 
First note that the variance of the prior distribution for $\theta_{t}$  is the sum of the posterior variance of $\theta_{t - 1}$ plus the evolution variance $W_{t}$.
Thus, the evolution variance serves to increase the uncertainty of the state above the initial uncertainty $C_{t}$.
Instead of keeping $W_{t}$ fixed, represent $W_{t}$ as a proportion of the posterior variance of the previous period.%
\footnote{
  This can also be derived from the local level model using limiting results when $V$ and $W$ are constant. 
  In the limit, $R = C / (1 - A)$.
  Thus, in the limit, $W$ is a fraction of the posterior state variance, $W = \frac{A}{1 - A} C$.
}
Let $W_{t} = \frac{\delta}{1 - \delta} C_{t - 1}$ where $\delta \in [0, 1]$. 
This implies that $R_{t} = C_{t - 1} / \delta$.
Thus the filtered model with discount factors consists of equations \eqref{dlm:eq:23}, \eqref{dlm:eq:30}, and
\begin{equation}
  \label{dlm:eq:10}
  \theta_{t} | y_{1:t-1} \sim N(m_{t-1}, R_{t} = \delta C_{t - 1})
\end{equation}
The discount factor $\delta$ represents the extent to which uncertainty about the state has increased from $t$ to $t - 1$, or conversely the extend to which information about the state has decayed in one period.
When $\delta \approx 0$, $A \approx 1$ and $m_{t} \approx y_{t}$.


