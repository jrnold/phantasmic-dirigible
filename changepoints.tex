
\section{Introduction}
\label{dlm:sec:introduction}

This paper 


\begin{itemize}
\item How change-points are typically 
\item Representation as a variable selection problem
\item Discussion of shrinkage priors
\item Dynamic linear model representation
\end{itemize}

Political and social processes are rarely, if ever, constant over time.
Thus, political scientists often have a need to estimate time-varying parameter (TVP) models.
There exist two broad approaches to estimating time-varying parameters: structural break approaches, which include dummy variables and change point models, and smoothing approaches, which include dynamic linear models and smoothing splines. 
Both approaches estimate TVP when the number of observations is small relative to the number of time periods. 
Smoothing approaches keep the parameter differences small in all periods, while structural break approaches restrict parameter changes to a few locations. 
These two approaches are often viewed as distinct and estimated using different methods, forcing the researcher to choose between which model to use.
Many processes the changes in the process are characterized by many periods of stability and a few periods of possibly rapid and large, change (Ratkovic and Eng 2010).
Historically, existing smoothing methods have had a difficulty estimating these sorts of processes, so structural break models have been used

This paper presents a simple and extensible method to estimate time-varying parameters that may be subject to structural breaks. 
Time-varying parameters with possible structural breaks can be estimated within a continuous state-space model, and in particular as a dynamic linear model, by placing a shrinkage prior on the distribution of the state disturbances.
The intuition behind this can be illustrated with a simple model.
Suppose there is a vector of observed data, $y_{1}, \dots, y_{n}$, drawn from a normal distribution with a time varying mean, $\mu_{1}, \dots, \mu_{n}$.
This can be represented as a dynamic linear model,

The difference between "structural breaks" and "smoothing" data-generating processes and estimation techniques is whether the XXXX vector is sparse (most XXXX = 0) or dense (most XXXX neq 0).
A common model is to apply a normal distribution to XXXX
A normal XXXX  estimate change over time, but it cannot produce sparse estimates of XXXX.
The thin tails of the normal distribution will tend to over-smooth periods around structural breaks, under-smooth periods in which there were no structural breaks.
However, estimating sparse parameter vectors is a general problem that has received considerable attention lately in large-p, small-n problems (Tibshirani1996 1996; Polson and Scott 2010). This paper applies some of the advances in sparse parameter estimation to estimating time-varying parameters with structural breaks. Structural breaks, i.e. sparse, can be estimated by placing a shrinkage prior on.
While there are a large number of Bayesian shrinkage priors proposed, this paper will use the Horseshoe prior (HS) distribution introduced in Carvalho, Polson, and Scott (2009) and Carvalho, Polson, and Scott (2010). I will refer to a dynamic linear models with shrinkage priors on the state disturbances as a sparse disturbance dynamic linear models (SDDLM).

This method does not require specifying the number of structural breaks ex ante.
\begin{enumerate}
\item The sparsity of XXXX will determine the number of structural breaks, and this sparsity can be estimated from the data.
Structural breaks can be detected from the posterior distribution of XXX using several rules. Not only do
the number of structural breaks need not be specifieded beforehand, this method will work reasonable
well even if the underlying data-generating process is dense, \ie{}the parameter changes in each period.
\item 2. This method is extensible. 
  Dynamic linear models incorporate a wide variety of models, including ARIMA
  and structural time-series, cubic splines, and regressions with time-varying coefficients. Any model in
  which the parameter of interest can be expressed as a latent state in a dynamic linear model can be
  altered to assign a shrinkage prior to the state disturbance in order to make it robust to or to detect
  structural breaks for that parameter. While many structural break methods are specific to single
  models, this paper shows how sparse-disturbance DLMs can be estimated within the general purpose
  Bayesian software, \Stan{}.
\item The method allows for easy estimation of structural breaks in multiple parameters which can be either
independent or correlated. Estimating independent structural breaks in a discrete state space model,
like change-point models, results in the state-space multiplying exponentially.
assigned a shrinkage prior.
\item This method is efficient in both programmer and computational time, while still retaining the extensibility
to estimate a wide variety of models. Since most shrinkage priors, including the HS distribution
used in the paper, are scale-mixture of normal distributions, this state space model estimated is a
(conditionally) Gaussian dynamic linear model (GDLM). There are can take advantage of the computationally
efficient methods of mode finding and sampling from GDLMs, such as the Kalman filter and
Forward-Filter Backwards-Sample. This paper shows how a combination of Stan, a general purpose
Bayesian software program, and R can be used to easily estimate and sample from the posterior of
dynamic linear models.
\end{enumerate}

Political science paper on change-points \parencites{CalderiaZorn1998}{WesternKleykamp2004}{Spirling2007a}{Spirling2007b}{Park2010}{Park2011}{Blackwell2012}.

\section{Change points as a Variable Selection Problem}
\label{dlm:sec:chang-as-vari}

For simplicity, I start with the case of change-points in the mean, and later generalize to other cases.
There are $n$ observations, $y_{1}, \dots, y_{n}$, with a time-varying mean, $\mu_{t}$, that takes $M$ values, $\mu^{*}_{1}, \dots, \mu^{*}_{M}$,  with change-point locations, $\tau_{1}, \dots, \tau_{m}$,
\begin{equation}
  \label{dlm:eq:1}
  \begin{aligned}[t]
    y_{t} & = \mu_{t} + \epsilon_{t} && \text{$\epsilon_{t}$ are iid with $\E(\epsilon) = 0$.} \\
    \mu_{t} &= \mu^{*}_{m} && \text{if $\tau_{m} \leq t \leq \tau_{m + 1}$, $\mu_{1} = \mu^{*}_{1}$, $\mu_{n} = \mu^{*}_{M}$.}
  \end{aligned}
\end{equation}
There are a variety of change-point approaches to this problem in both classical \parencites{Page1954a}{Hinkley1970a}{BaiPerron2003a}{OlshenVenkatramanLucitoEtAl2004}{BaiPerron1998}{KillickFearnheadEckley2012} and Bayesian statistics \parencites{Yao1984}{BarryHartigan1993}{Chib1998}{Fearnhead2006a}{FearnheadLiu2007a}.


An alternative approach is to rewrite the problem in equation (\ref{dlm:eq:1}) to focus on the values of the changes in the mean (innovations), $\omega_{t} = \mu_{t} - \mu_{t - 1}$, rather than the locations of the change points:
\begin{equation}
  \label{dlm:eq:2}
  \begin{aligned}[t]
    y_{t} &= \mu_{t} + \epsilon_{t} && \text{$\epsilon_{t}$ are iid with $\E(\epsilon) = 0$.} \\
    \mu_{t} &= \mu_{t - 1} + \omega_{t}
  \end{aligned}
\end{equation}
In a change points situation, the innovations $\omega_{t}$ are sparse, meaning that most values of $\omega_{t}$ are zero, and only a few are non-zero.
The times of the change points are not directly estimated, but are those times at which $\omega_{t}$ are non-zero.
This turns the problem from one of segmentation, to one of variable selection, in which the goal is to find the non-zero values of $\omega_{t}$ and estimate their values.
\footnote{In Equation (\ref{dlm:eq:2})The initial value of $\mu_{1}$, or equivalently $\mu_{0}$, needs to be given a value.
%This will likley work better with the formulation that initilizes at t = 1, since we should assume \omega_1=0.
}
The natural approach to this variable selection problem is to explicitly model the values of $\omega$ as a discrete mixture between a point mass at zero for the non-change points, and a heavy-tailed alternative \textcite{MitchellBeauchamp1988a}{Efron2008a}:
\begin{equation}
  \label{dlm:eq:3}
  \omega_{t} = \pi g(\omega) + (1 - \pi) \delta_{0}
\end{equation}
where $g$ is some heavy-tailed distribution.
Equation (\ref{dlm:eq:3}) is often called a ``spike and slab'' prior \textcite{MitchellBeauchamp1988a}.
Since it explicitly models the two groups (zeros and non-zeros), \textcite{Efron2008a} calls this the two-group answer to the two-groups problem.
This prior is convenient because it directly provides for each time a posterior probability that it is a change point (non-zero).
\textcite{GiordaniKohn2008} propose using the representation in Equation (\ref{dlm:eq:2}) with the a discrete mixture distribution for $\omega$, as in Equation (\ref{dlm:eq:3}), to estimate change points.

Recent work in Bayesian computation has focused on one-group solutions to the variable selection problem, which combine shrinkage (regularization) and variable selection through the use of continuous distribution with a large spike at zero and wide tails \parencite{}
This is analagous to the sparse regularization literature in frequentist statistics spawned by LASSO \parencite{Tibshirani1996} and its the numerous variations.
In this approach, several papers have proposed using LASSO-type penalities in a maximum likelihood approach to estimate change-points \parencites{TibshiraniEtAl2005}{HarchaouiLevy-Leduc2010}{ChanYauZhang2014}.
In the Bayesian literature or shrinkage in sparse situations numerous distributions have been proposed. 
This paper will look at four of them.

\begin{itemize}
\item Double Exponential
\item Horseshoe
\item Horseshoe+ 
\end{itemize}

Shrinkage priors are the Bayesian equivalent of penalized likelihood and regularized regression.
Most proposed Bayesian shrinkage priors are in the the class of global-local mixtures of normal distributions \parencite{PolsonScott2010}:
\begin{equation}
  \label{dlm:eq:6}
  \begin{aligned}[t]
    \theta_{t} &= N(0, \tau^{2} \lambda_{i}^{2}) \\ 
    \lambda_{t}^{2} &\sim p(\lambda_{t}^{2}) \\
    \tau_{t}^{2} &\sim p(\tau^{2})
  \end{aligned}
\end{equation}
A global-local normal distribution is a scale mixture of normal distributions in which the variance is the product of a global variance component $\tau^{2}$ and a local variance component $\lambda_{t}^{2}$.
The $\lambda_t$ are also called mixing parameters, and their distribution is called the mixing distribution.
Distributions in the global-local class of priors differ in the assumed distribution of the local variance component. 
Examples of commonly used shrinkage distributions and their mixing distributions include \parencite{PolsonScott2010}:%
\footnote{A normal prior, as used in ridge regression, is the trivial case in which $\lambda_{i} = 1$ for all $i$.}

\begin{description}
\item[Student's $t$] The mixing distribution is inverse gamma, $p(\lambda_{i}) = \dinvgamma{\xi / 2, \xi \tau^{2} / 2}$ where $\xi$ are the degrees of freedom. \parencite{Tipping2001}
\item[Double Exponential (Laplacian)] The mixing distribution is exponential \parencites{ParkCasella2008}{Hans2009},
\begin{equation}
  \begin{aligned}[t]
    p(\lambda_{i} | \tau_{i}) &= \frac{1}{2 \tau^{2}} \exp (\lambda_{i}^{2} / 2 \tau^{2}) \\
    p(\tau^{2}) &\sim \dinvgamma{\xi / 2, \xi d^{2} / 3}
  \end{aligned}
\end{equation}
\item[Horseshoe] The mixing distribution is inverse-beta $\lambda^{2}_{i} \sim \dinvbeta{\frac{1}{2}, \frac{1}{2}}.$ 
Equivalently, $\lambda_{i} \sim \dhalfcauchy{0, 1}$ \parencite{CarvalhoPolsonScott2010}.
\item[Horseshoe+] 
\end{description}

While this one-group approach does not provide as clear an estimate of the locations or existence of change points, it is useful for several reasons.
\begin{enumerate}
\item For many time-varying parameter processes typically modeled with a change-point model, the situation akin to a situation in which the parameter is changing in all periods, but most of these changes are small relative to that of a few periods.
  In other words, the innovations $\omega$ are never zero; it is just that in most periods they are relatively small.
  This seems plausible for many processes modeled by political scientists in which there is no physical reason to think there are discrete states, apart from perhaps institutional changes.
  It may be useful for the researcher to model it with a step function (change points) for parsimony and interpretation, but the data-generating process is likely closer to one in which things are usually changing a little, but in some periods they change a lot. 
  I would argue that the data generating processes considered by political science papers using change points falls into this category, for example Supreme court dissent and consensus \parencite{CalderiaZorn1998}, wage growth in OECD states \parencite{WesternKleykamp2004}, casualties in the Iraq War \parencite{Spirling2007a}, presidential use of force \parencite{Park2010}, and campaign contributions \parencite{Blackwell2012}.
\item When the goal is estimation or prediction of the parameters, rather than the locations of the change-points, then the changes in the parameter is continuous anyways after integrating over the posterior distribution of the change-points. 
  The one-group approach approximates this distribution.
\end{enumerate}

\section{Estimation}
\label{dlm:sec:estimation}



\subsection{Dynamic Linear Models}
\label{dlm:sec:dynam-line-models}

A dynamic linear model, also called Gaussian state space models, is represented by a system of equations
\begin{equation}
  \label{dlm:eq:4}
  \begin{aligned}[t]
  y_{t} &\sim \dnorm{b_{t} + F_{t} \theta_{t}, V_{t}} \\
  \theta_{t} &\sim \dnorm{g_{t} + G_{t} \theta_{t - 1}, W_{t}} 
  \end{aligned}
\end{equation}
This is covered in INSERT TEXTS.

The model in Section \Stan{}, a probabilistic programming language, through the \RLang{} package \textbf{rstan}.
These models could be estimated in \Stan{} directly by translating the models described in the previous section into a \Stan{} model.
Although the Horseshoe and Horseshoe+ distributions are not directly implemented in \Stan{}, they are easily implemented in Stan through their hierarchical representation.%
\footnote{See the examples in \url{http://andrewgelman.com/2015/02/17/bayesian-survival-analysis-horseshoe-priors/} and \url{http://becs.aalto.fi/en/research/bayes/diabcvd/}.}

However, Gaussian state space models can be estimated more efficiently by marginalizing over the latent states while sampling other parameters, and then sampling the latent states using a Forward-Filter Backwards Sampler (FFBS) type.

Several \RLang{} packages implement Kalman filtering, smoothing, and simulation for Gaussian state space models; see \textcite{Tusell2011} for an overview.
However, these do not provide a full Bayesian sampling solution, since the user would still need to write their own sampler for any other parameters going into the state space model, or if the state space model were a component of a more complex model.
The  probabilistic programming language, \proglang{JAGS}, does not implement a specific sampler for Gaussian linear state space models.

Let $\vartheta$ refer to all parameters in the system vectors and matrices in the state space models,
\begin{enumerate}
\item Sample $\vartheta$ from $p(\vartheta | y)$ using HMC in \Stan{}.
  This requires integrating out the latent states, $\theta$, and calculating $p(y | \vartheta)$.
  The value of $p(y | \vartheta)$ is calculated using the Kalman filter.
\item Sample $p(\theta | y, \vartheta)$ using a simulation sampler for the linear Gaussian state space model.
\end{enumerate}
I use this method for all models in the paper. 
To implement this method in \Stan{}.
I provide a full set of user-defined \Stan{} functions that implements Kalman filters, smoothers, and simulation sampling.
Section XXXX provides documentation of these functions.



\section{Change Point Examples}
\label{dlm:sec:examples}


\begin{itemize}
\item Blocks
\item Nile
\item Civil War Greenbacks / Graybacks
\end{itemize}






\section{Efficient Estimation of DLMs in Stan}
\label{dlm:sec:effic-estim-dlms}

Comparison of Kalman filter FFBS implementations: @DurbinKoopmans2012
Other comparisons of sampling schemes: @ReisSalazarGamerman2006.
Comparison of speed of implementations in R, focusing on mode finding: @Tusell2011.


Marginalizing over latent states is necessary in discrete state space models in \Stan{} since HMC does cannot sample discrete parameters CITE.


%  LocalWords:  iid Efron GiordaniKohn2008 Tibshirani1996 Tipping2001
%  LocalWords:  PolsonScott2010 ParkCasella2008 Hans2009 DLMs Kalman
%  LocalWords:  CaronDoucet2008 BrownGriffin2010 FFBS Tusell2011 HMC
%  LocalWords:  CarvalhoPolsonScott2010 DurbinKoopmans2012 TVP Polson
%  LocalWords:  ReisSalazarGamerman2006 Ratkovic Carvalho SDDLM XXXX
%  LocalWords:  ARIMA GDLM GDLMs CalderiaZorn1998 WesternKleykamp2004
%  LocalWords:  Spirling2007a Spirling2007b Park2010 Park2011 OECD
%  LocalWords:  Blackwell2012 MitchellBeauchamp1988a Efron2008a rstan
%  LocalWords:  Graybacks
