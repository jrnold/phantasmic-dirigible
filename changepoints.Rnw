<<echo = FALSE, results = 'hide', message = FALSE, warning = FALSE, error = FALSE>>=
library("bsdlm")
data("BushApproval", package = "bsdlm")
tab_path <- file.path(PROJECT_ROOT, "analysis", "tex")
fig_path <- file.path(PROJECT_ROOT, "analysis", "figures")
date_format <- function(x) format(x, "%B %d, %Y")


@ 
\newcommand{\ModelII}[1]{\texttt{#1}}


\section{Introduction}
\label{sec:introduction}

Political and social processes are rarely, if ever, constant over time, and as such researchers have a need to model that change \parencites{Buethe2002a}{Lieberman2002a}.
Moreover, these political process are often marked by both periods of stability and periods of large changes \parencites{Pierson2004}.
If the researcher has a strong prior about or wishes to test a specific location of a change, they may include indicator variables, an example in international relations being the ubiquitous Cold War dummy variable.
Other approaches estimate locations of these change using change point or structural break models \parencites{CalderiaZorn1998}{WesternKleykamp2004}{Spirling2007a}{Spirling2007b}{Park2010}{Park2011}{Blackwell2012}.
This offers a different Bayesian approach to modeling change points.
I combine a continuous latent state space approach \parencite{Beck1989}, with recent advances in Bayesian shrinkage priors \parencites{CarvalhoPolsonScott2009}{CarvalhoPolsonScott2010}{PolsonScott2010}.
These sparse shrinkage priors are the Bayesian analog to the advances in sparse regularization such as the LASSO \parencites{Tibshirani1996}.
For example, to model change points the mean, I use the following model of observations with a time-varying mean.
\begin{equation}
  \label{eq:31}
  \begin{aligned}[t]
  y_{t} &= \mu_{t} + \epsilon_{t} & \text{$\epsilon$ iid, $\E(\epsilon) = 0$} \\
  \mu_{t} &= \mu_{t - 1} + \omega_{t}
  \end{aligned}
\end{equation}
To model change points, I model the change in the mean, $\omega_{t}$, with a shrinkage prior distribution that has most of its mass at zero but has wide tails.
This will generate estimates of $\omega$ in which most times are close to zero, but a few times, the change points, can be large.
The estimated posterior distribution of the series of $\mu$ will resemble a step function.
The particular shrinkage priors that will be used in this work are the the horseshoe and horseshoe+ distributions \parencites{CarvalhoPolsonScott2009}{CarvalhoPolsonScott2010}{BhadraDattaPolsonEtAl2015a}.

A complementary contribution of this work is that it provides a method to efficiently estimate Gaussian dynamic linear models (GDLMs) using a general purpose Bayesian programming language, \Stan{}.
Gaussian dynamic linear models are a class of models than incorporates many common time series models, including ARIMA, structural time series, and linear regression with time-varying parameters.
These had previously been difficult to sample in general purpose Bayesian programming languages \parencite[477]{Jackman2009}.
This work provides a complete set of functions to perform Kalman filtering, smoothing, and backward sampling within \Stan{}.
This allows for efficiently estimating DLMs in \Stan{} using a partially collapsed Gibbs sampler in which \Stan{}'s standard algorithms are used to estimate parameters after marginalizing over the latent states, and then sample the latent states of the DLM using backwards sampling.


This method does not require specifying the number of structural breaks ex ante.
The sparsity of XXXX will determine the number of structural breaks, and this sparsity can be estimated from the data.
Structural breaks can be detected from the posterior distribution of XXX using several rules. Not only do
the number of structural breaks need not be specifieded beforehand, this method will work reasonable
well even if the underlying data-generating process is dense, \ie{}the parameter changes in each period.

This method is extensible. 
Dynamic linear models incorporate a wide variety of models, including ARIMA
and structural time-series, cubic splines, and regressions with time-varying coefficients. Any model in
which the parameter of interest can be expressed as a latent state in a dynamic linear model can be
altered to assign a shrinkage prior to the state disturbance in order to make it robust to or to detect
structural breaks for that parameter. While many structural break methods are specific to single
models, this paper shows how sparse-disturbance DLMs can be estimated within the general purpose
Bayesian software, \Stan{}.

The method allows for easy estimation of structural breaks in multiple parameters which can be either
independent or correlated. Estimating independent structural breaks in a discrete state space model,
like change-point models, results in the state-space multiplying exponentially.
assigned a shrinkage prior.

This method is efficient in both programmer and computational time, while still retaining the extensibility
to estimate a wide variety of models. Since most shrinkage priors, including the HS distribution
used in the paper, are scale-mixture of normal distributions, this state space model estimated is a
(conditionally) Gaussian dynamic linear model (GDLM). There are can take advantage of the computationally
efficient methods of mode finding and sampling from GDLMs, such as the Kalman filter and
Forward-Filter Backwards-Sample. This paper shows how a combination of Stan, a general purpose
Bayesian software program, and R can be used to easily estimate and sample from the posterior of
dynamic linear models.

First, if change points could be understood in a weaker sense. 
In few political processes is there actually no change in periods.
Instead, change is non-zero, but slow in most periods, with some periods showing rapid change.

Second, even though change point models directly estimate the location of the change points, after averaging over the posterior probability of the change points, the posterior distribution of the changes in the parameter will not be sparse. 

Third, using these sorts of one-group models may have some benefits in either flexibility in terms of the types of models which can be estimated, or in the computational speed.

Note that these models, like many sparsity inducing models, often conflate two distinct objectives. 
The first goal of these models is to improve in- or out-of-sample fit which the data generating process actually has a few very large signals and many zero or relatively small signals.
The second goal of these models is interpretation for the researcher.
It is easier to present results that ``there is a change point at time $t$'' than ``the data is slowly trending, and it seems to be trending sharply around $t$, but then it does not move as much after that.''
In this case, sparsity is not (directly) an attempt to model the data generating process, but a means to get a more interpretable model.
In many contexts performing statistical inference on ``how many change points?'' or ``what is the probability that $t$ is a change point?'' is non-sensical, since it is almost certain that there a no ``true'' change points, and finding the right number of change points is the researcher choosing a tradeoff between how well the model represents a complex reality, and how interpretable the model is.
The method presented here does not produce perfectly sparse models, nor does provide probabilities that certain points are or are note change points. 
Therefore, it is better suited to the first case, of modeling cases in which the 


\section{Change points as a Variable Selection and Shrinkage Problem}
\label{sec:chang-as-vari}

For simplicity, I start with the mode of change points in the mean, and later generalize to other cases.
In this case, there are $n$ ordered observations, $y_{1}, \dots, y_{n}$, with a time-varying mean, $\mu_{t}$.
\begin{equation}
  \label{eq:17}
  \begin{aligned}[t]
    y_{t} & = \mu_{t} + \epsilon_{t} && \text{$\epsilon_{t}$ are iid with $\E(\epsilon) = 0$.} \\  
  \end{aligned}
\end{equation}
Suppose that there are $M$ change points, with ordered change-point locations, $\tau_{1}, \dots, \tau_{m}$, with the convention that $\tau_{0} = 0$ and $\tau_{M + 1} = n$.
This splits the mean into $M$ segments, with values of the mean $\mu^{*}_{1}, \dots, \mu^{*}_{M}$,  such that
\begin{equation}
  \label{eq:1}
  \begin{aligned}[t]
    \mu_{t} &= \mu^{*}_{m} && \text{if $\tau_{m} \leq t \leq \tau_{m + 1}$}
  \end{aligned}
\end{equation}
There are a variety of approaches to the change point problem in both classical (frequentist) \parencites{Page1954a}{Hinkley1970a}{BaiPerron2003a}{OlshenVenkatramanLucitoEtAl2004}{BaiPerron1998}{KillickFearnheadEckley2012} and Bayesian statistics \parencites{Yao1984}{BarryHartigan1993}{Chib1998}{Fearnhead2006a}{FearnheadLiu2007a}.

An alternative approach to the change point problem is to rewrite the problem in Equation (\ref{eq:1}) to focus on the changes in the mean (system errors), $\omega_{t} = \mu_{t} - \mu_{t - 1}$, rather than the locations of the change points:
\begin{equation}
  \label{eq:2}
  \begin{aligned}[t]
    \mu_{t} &= \mu_{t - 1} + \omega_{t}
  \end{aligned}
\end{equation}
In a change point model, the system errors, $\omega_{t}$, are sparse, meaning that most values of $\omega_{t}$ are zero, and only a few are non-zero.
In this formulation, although times of the change points are not directly estimated, the change points are those times at which $\omega_{t}$ are non-zero.
This formulation turns the problem from one of segmentation, to one of variable selection, in which the goal is to find the non-zero values of $\omega_{t}$ and estimate their values.%
%\footnote{In Equation (\ref{eq:2}) the initial value of $\mu_{1}$, or equivalently $\mu_{0}$, needs to be given a value.
%This will likley work better with the formulation that initilizes at t = 1, since we should assume \omega_1=0.
%}
The natural approach to this variable selection problem is to explicitly model the values of $\omega$ as a discrete mixture between a point mass at zero for the non-change points, and an alternative distribution \parencites{MitchellBeauchamp1988a}{Efron2008a}:
\begin{equation}
  \label{eq:3}
  \omega_{t} = \rho g(\omega) + (1 - \rho) \delta_{0}
\end{equation}
where $g$ is a distribution of the non-zero $\omega$, and $\delta_{0}$ is a Dirac delta distribution (point mass) at 0.
Equation (\ref{eq:3}) is often called a ``spike and slab'' prior \parencite{MitchellBeauchamp1988a}.
Since it explicitly models the two groups (zeros and non-zeros), \textcite{Efron2008a} calls this the two-group answer to the two-groups problem.
This prior is convenient because it directly provides for each observation a posterior probability that it is a change point (non-zero).
\textcite{GiordaniKohn2008} propose using the representation in Equation (\ref{eq:2}) with the a discrete mixture distribution for $\omega$, as in Equation (\ref{eq:3}), to estimate change points.

Recent work in Bayesian computation has focused on one-group solutions to the variable selection problem, which combine shrinkage and variable selection through the use of continuous distribution with a large spike at zero and wide tails \parencite{PolsonScott2010}.
Numerous sparse sparse shrinkage priors have been proposed, but this paper will look at the Student's $t$  \parencite{Tipping2001}, the Laplace or double exponential \parencite{ParkCasella2008}{Hans2009}, the horseshoe \parencite{CarvalhoPolsonScott2010}, and horseshoe+ distributions \parencite{BhadraDattaPolsonEtAl2015a}.

\begin{description}

\item[Student's $t$] The Student's $t$ distribution for $x \in \R$  with scale $\tau \in \Rp$ and degrees of freedom $\nu \in \Rp$,
\begin{equation}
  \label{eq:6}
  p(\omega_{t} | \tau, \nu) = \frac{\Gamma\left(\frac{\nu + 1}{2}\right)}{\sqrt{\nu \pi} \Gamma\left( \frac{\nu}{2} \right)} \left( 1 + \frac{\omega_{t}^{2}}{\nu} \right)
\end{equation}
The Student's $t$ distribution can also be expressed as a scale mixture of normals,%
\footnote{
  $\dinvgamma{x | \alpha, \beta}$ is an inverse-gamma distribution for $x \in \Rp$ with shape $\alpha \in \Rp$ and inverse-scale $\beta \in \Rp$,
  \begin{equation}
    \label{eq:9}
    \dinvgamma{x | \alpha, \beta} = \frac{\beta^{\alpha}}{\Gamma(\alpha)} x^{-(\alpha - 1)} \exp 
    \left(
      \beta \frac{1}{x}
    \right)
  \end{equation}
}
\begin{equation}
  \label{eq:5}
  \begin{aligned}[t]
  \omega_{t} | \tau, \nu, \lambda_{t} &\sim \dnorm{0, \tau^{2} \lambda_{t}^{2}} \\
  \lambda_{t}^{2} | \nu & \sim \dinvgamma{\frac{\nu}{2}, \frac{\nu}{2}}
  \end{aligned}
\end{equation}
\parencite{Tipping2001} uses the Student's $t$ for sparse shrinkage by letting the degrees of freedom $\nu \to 0$.

\item[Double Exponential] The double exponential (Laplace) with scale $\tau$,
\begin{equation}
  \label{eq:14}
  p(\omega_{t}| \tau) = \frac{1}{2 \tau} \exp 
  \left(
    - \frac{\abs{\omega_{t}}}{\tau}
  \right)
\end{equation}
The double exponential distribution can also be expressed as a scale-mixture of normal distributions,%
\footnote{
  $\dexp{x | \beta}$ is the exponential distribution for $x \in \Rp$ with inverse-scale (rate) parameter $\beta \in \Rp$,
    \begin{equation}
      \label{eq:13}
      \dexp{x | \beta} = \beta \exp(- \beta x)
    \end{equation}
}
\begin{equation}
  \label{eq:12}
  \begin{aligned}[t]
  \omega_{t} | \tau, \lambda_{t} &\sim \dnorm{0, \tau^{2} \lambda_{t}^{2}} \\
  \lambda_{t}^{2} & \sim \dexp{\frac{1}{2}}
  \end{aligned}
\end{equation}
The double exponential distribution is the distribution that corresponds to the $\ell_{1}$ penalty used in LASSO \parencites{ParkCasella2008}{Hans2009}.
The LASSO achieves sparsity due to the spike in the double exponential distribution at zero places also places the mode of estimates at zero. 
However, the although the double exponential distribution has a spike at zero, it does not produce sparse posterior means since it has relatively little mass close to zero.
Additionally, the tails of the double exponential are narrow, so it tends to excessively shrink large signals \parencites{CarvalhoPolsonScott2010}.

\item[Horseshoe] The horseshoe distribution \parencite{CarvalhoPolsonScott2009}{CarvalhoPolsonScott2010} is defined hierarchically as a scale-mixture of normals
\begin{equation}
  \label{eq:7}
  \begin{aligned}[t]
    \omega_{t} | \lambda_{t}, \tau & \sim \dnorm{0, \tau^{2} \lambda_{t}^{2}} \\
    \lambda_{t}  & \sim \dhalfcauchy{0, 1}
  \end{aligned}
\end{equation}
where $\dhalfcauchy{x | 0, s}$ denotes half-Cauchy distribution with a scale parameter $s$, and density
\begin{equation}
  \label{eq:8}
  p(x | s) = \frac{2}{\pi x \left(1 + {\left(\frac{x}{s}\right)}^{2}\right)}
\end{equation}
The Horseshoe distribution has some theoretically attractive properties for shrinkage and variable selection \parencites{CarvalhoPolsonScott2009}{CarvalhoPolsonScott2010}{DattaGhosh2012}{PasKleijnVaart2014a}.

\item[Horseshoe+] The horseshoe+ distribution \textcite{BhadraDattaPolsonEtAl2015a} is similar to the Horseshoe distribution, but with an additional term,
\begin{equation}
  \label{eq:15}
  \begin{aligned}[t]
    \omega_{t} | \lambda_{t}, \eta_{t}, \tau & \sim \dnorm{0, \tau^{2} \lambda_{t}^{2}} \\
    \lambda_{t}  & \sim \dhalfcauchy{0, \eta_{t}} \\
    \eta_{t} & \sim \dhalfcauchy{0, 1} \\
  \end{aligned}
\end{equation}
\end{description}

The shrinkage distributions considered here also global-local scale mixtures of normal, with a global variance component $\tau$, and local variance components $\lambda_{t}$ \parencite{PolsonScott2010}.
The global variance component, $\tau$, concentrates the distribution around zero, while the local variance components, $\lambda_{t}$, allow individual values to be far from zero.
The choice of the prior distribution for the global variance component, $\tau$, is particularly important in these shrinkage distributions as it effectively controls the sparsity of the estimates, number of change points in this example.
As per the suggestion in \textcite{BhadraDattaPolsonEtAl2015a} and \textcite{PasKleijnVaart2014a}, throughout this work, I will use the prior distribution
\begin{equation}
  \tau \sim \dhalfcauchy{0, \frac{1}{n}}
\end{equation}
where $n$ is the number of observations.
\footnote{
  Other suggestions have been $\tau \sim \dhalfcauchy{0, 1}$, $\tau \sim \dunif{0, 1}$ where $\dunif{}$ is the uniform distribution, and a plug-in value of $p / n$, where $p$ is the expected number of non-zero parameters. See \textcites{PolsonScott2012}{PasKleijnVaart2014a}{BhadraDattaPolsonEtAl2015a}.
}
That these are distributions can all be expressed as normal distributions, conditional the values of $\lambda_{t}$ and $\tau$, is useful computationally, as is discussed in Section \ref{sec:estimation}.

These Bayesian sparse shrinkage priors are analogous to the sparse regularization in frequentist statistics of which LASSO \parencite{Tibshirani1996} and its the numerous variations is the most prominent example.
Several papers have proposed using LASSO-like penalties and maximum likelihood to estimate change-points \parencites{TibshiraniEtAl2005}{HarchaouiLevy-Leduc2010}{ChanYauZhang2014}.
This is a Bayesian extension of that approach.


To recap, the proposed model for change points is
\begin{align}
  \label{eq:16}
  y_{t} & \sim \epsilon_{t} & \text{$\epsilon_{t}$ iid, $\E(\epsilon) = 0$, $\Var(\epsilon) = \sigma^{2}$} \\
  \mu_{t} &= \mu_{t - 1} + \sigma \omega_{t}
\end{align}
where $\omega_{t}$ is given a shrinkage prior distribution that induces sparsity.
This work will propose horseshoe and horseshoe+ distributions as those shrinkage priors, and compare them other shrinkage priors such as the Student's $t$ and double exponential distributions.


\begin{figure}[!htpb]
 \begin{subfigure}[b]{\linewidth}
   \includegraphics{\Sexpr{file.path(PROJECT_ROOT, "analysis", "figures", "dist-plot_zeros-1")}}
   \caption{Near zero.}
 \end{subfigure}
 \begin{subfigure}[b]{\linewidth}
    \includegraphics{\Sexpr{file.path(PROJECT_ROOT, "analysis", "figures", "dist-plot_tails-1")}}
    \caption{Tail region.}
 \end{subfigure}
  \caption[Comparison of the density functions of normal, Cauchy, Laplace, horseshoe, and horseshoe+ distributions.]{
    Comparison of the density functions of normal, Cauchy, Laplace, horseshoe, and horseshoe+ distributions.
    All functions have location and scale parameters of 0.
    The Cauchy distribution is a special case of the Student's $t$ distribution with degrees of freedom equal to 1.
  }
  \label{fig:density}
\end{figure}




\section{Discussion}
\label{sec:ident-change-points}

The method proposed here is better at estimating the values of $\mu_{t}$, or rather the posterior distribution, when change-points may be present than providing probabilities of the locations of the change-points themselves.
The first method uses the posterior distribution to detect structural breaks is simply a test using posterior distribution of $\omega$.
This is the Bayesian equivalent to the auxiliary residual test of \textcite{DeJongPenzer1998}.
In addition to the use of a credible interval rather than a confidence interval, the Bayesian method differs from the auxiliary residual test in that it integrates over the posterior distribution of the state matrices rather than fixing them at point estimates.


% The second method uses the local variance components $\lambda_{t}$ to identify structural breaks \parencite[179-180]{PetrisPetroneEtAl2009}.
% If $\lambda_{t}$ = 1, then the state disturbance is distributed normal with a scale equal to the global scale.
% Thus, the local shrinkage parameters $\lambda_{t}$ are relative measure of how much each $\omega_{t}$ is shrunk towards zero.
% Values of $\lambda_{t} > 1$ ($\lambda_{t} < 1$ indicate state disturbances that are dispersed (shrunk) relative to the global scale.
% A structural break can be classified as a time period in which $\E(\lambda_{t}) > 1$.

% The third method uses the contribution of observations to the state. 
% Talk about Kalman gain and smoother

Finally, for many practical purposes and interpretation, an ``eyeball test'' by the researcher by plotting the mean or median of the posterior estimates of $\mu$ should suffice.As noted earlier, for many data generating processes, the hypothesis that $\omega_{t} = 0$ is implausible, so testing for change points does not make much sense.
Rather, change point models are used to ease interpretation, so an informal method of visualizing is enough.
If $\Delta \mu$ is not distinguishable in a plot, then it is unlikely to be of any practical import.

While this one-group approach does not provide as clear an estimate of the locations or existence of change points, it is useful for several reasons.
\begin{enumerate}
\item For many time-varying parameter processes typically modeled with a change-point model, the situation akin to a situation in which the parameter is changing in all periods, but most of these changes are small relative to that of a few periods.
  In other words, the system errors $\omega$ are never zero; it is just that in most periods they are relatively small.
  This seems plausible for many processes modeled by political scientists in which there is no physical reason to think there are discrete states, apart from perhaps institutional changes.
  It may be useful for the researcher to model it with a step function (change points) for parsimony and interpretation, but the data-generating process is likely closer to one in which things are usually changing a little, but in some periods they change a lot. 
  I would argue that the data generating processes considered by political science papers using change points falls into this category, for example Supreme court dissent and consensus \parencite{CalderiaZorn1998}, wage growth in OECD states \parencite{WesternKleykamp2004}, casualties in the Iraq War \parencite{Spirling2007a}, presidential use of force \parencite{Park2010}, and campaign contributions \parencite{Blackwell2012}.
\item When the goal is estimation or prediction of the parameters, rather than the locations of the change-points, then the changes in the parameter is continuous anyways after integrating over the posterior distribution of the change-points. 
  The one-group approach approximates this distribution.
\end{enumerate}


\section{Estimation and Implementation in \Stan{}}
\label{sec:estimation}

A helpful feature of the model as laid out in the previous section is that it that it can be expressed as Gaussian dynamic linear model, which has properties useful for its computation and generalization.
A Gaussian dynamic linear model (GDLM), or linear Gaussian state space model, is a model represented by the system of equations \parencites{DurbinKoopman2012}{WestHarrison1997}{PetrisPetroneEtAl2009}[Ch 6]{ShumwayStoffer2010},
\begin{align}
  \label{eq:4}
  y_{t} &\sim \dnorm{b_{t} + F_{t} \theta_{t}, V_{t}} \\
  \label{eq:24}
  \theta_{t} &\sim \dnorm{g_{t} + G_{t} \theta_{t - 1}, W_{t}} 
\end{align}
In these equations, the observed date , $y_{t}$, is a linear function of some latent states, $\theta_{t}$, which are a function of their previous values $\theta_{t-1}$.
Equation (\ref{eq:4}) is the \textit{observation equation}, where $y_{t}$ (observation vector) is a $r \times 1$ vector of observed data, $\theta_{t}$ (state equation) is a $p \times 1$ vector of unobserved states, $b_{t}$ is an $r \times 1$ vector, $F_{t}$ is a $r \times p$ matrix, and $V_{t}$ (observation variance) is an $r \times r$ covariance matrix.
Equation (\ref{eq:24}) is the \textit{state equation} equation, which relates the current state to its previous value; $g_{t}$ is an $p \times 1$ vector, $G_{t}$ is a $p \times p$ matrix, and $W_{t}$ (state variance) is an $p \times p$ covariance matrix.
The vectors and matrices, $\Phi = \{ b_{t}, g_{t}, F_{t}, G_{t}, V_{t}, W_{t} \}_{t \in 1:n}$ are \textit{system matrices}, which in applications will often be functions of parameters.
GDLMs are a general class of models which includes many common time series models, including SARIMA models, structural time series \parencite{Harvey1990}, dynamic factor models, seemingly unrelated regression, and linear regression with time varying coefficients, among others \parencite[Ch. 3]{DurbinKoopman2012}. 

The model in Section \ref{sec:chang-as-vari}, Equations \eqref{eq:17} and \eqref{eq:2}, is a GDLM.
In this case, $\beta_{t} = g_{t} = 0$, $F_{t} = G_{t} = 1$, and $\epsilon_{t}$ in Equation (\ref{eq:17}) are distributed $\epsilon_{t} \sim \dnorm{0, V_{t}}$, and $\omega_{t} \sim \dnorm{0, V_{t}}$.
Even though the proposed distributions distributions for $\omega_{t}$ are not normal, since they are all representable as scale-mixtures of normal distributions, they are normal conditional on the values of $\lambda_{t}$.%
These models are similar to what is often called the \textit{local level model} \parencite[Ch 2.]{DurbinKoopman2012}, except that they have time-varying state variances, $W_{t}$.
In other words, the change point model discussed here is simply a local level model with a sparsity-inducing shrinkage prior on the state variance.

That these change point models can be represented as linear Gaussian state space models is useful for two reasons.
First, it suggests how these models can be extended beyond simple changes in means.
Any sort of GDLM, which includes many common models, can be adjusted to account for change points in in any of its states, by using a shrinkage prior distribution for its system variance, $W$.%
\footnote{Though the shrinkage prior distribution will need to be a scale-mixture of normals.}

Second, since both the observation and system equations are multivariate normal, there are analytical solutions that allow for efficiently computing its likelihood and sampling the latent states from their posterior distribution.
The Kalman filter calculates the values of $p(\theta_{t} | y_{1:(t-1)})$  and $(\theta_{t}| y_{1:t})$, and can be used to calculate the likelihood of $p(y | \Phi)$.
Importantly, the Kalman filter calculate the likelihood of $y$ without needing the values of the latent states $\theta$.
The derivations of the Kalman filter can be found in most time-series texts, including \textcite[Ch. 5--7]{DurbinKoopman2012}{WestHarrison1997}.
Given the results of the Kalman filter there are several methods to sample $\theta$ from $p(\theta | y, \Phi)$, a process called Forward-Filtering Backwards-Smoothing (FFBS) or simulation smoothing \parencites{CarterKohn1994}{Fruehwirth-Schnatter1994}{DeJongShephard1995}{DurbinKoopman2002}[Ch 4.9]{DurbinKoopman2012}.
I make use of these methods to efficiently sample both the latent states, $\theta$, and other parameters of the model in \Stan{}.

\Stan{} is a probabilistic programming language, with its own BUGS-like language, and interfaces to several programming languages, including \RLang{} \parencites{Stan2015a}{CarpenterGelmanHoffmanEtAl2015a}.
GDLMs could be estimated in \Stan{} by directly translating the models described Equations (\ref{eq:4}) and (\ref{eq:24}) into a \Stan{} model.%
\footnote{Since these models are hierarchical, it will be more efficient to implement them using the computational tricks in Section ... of X.}
Although the horseshoe and horseshoe+ distributions are not directly implemented in \Stan{} since they have not closed form, but they are easily implemented in Stan through their hierarchical representation.

However, GDLMs can be estimated more efficiently in \Stan{} by marginalizing over the latent states.
The sampling methods implemented in \Stan{}, the primary of which is HMC-NUTS \parencites{HoffmanGelman2014a}, only require the calculation of a likelihood from the user.%
\footnote{Technically, it also requires derivatives of the likelihood, but these are generated automatically by \Stan{} using its automatic differentiation engine.}
The Kalman filter can be used to calculate the likelihood $p(y | ...)$ by marginalizing over the latent states, $\theta_{t}$. 
Marginalizing over parameters is required when estimating models with discrete parameters in \Stan{}, e.g. mixture models \parencite[104]{Stan2015a}.
Although it is not necessary to sample from GDLMs, it helps the efficiency of sampling by reducing the correlation between the latent states and the other parameters of the model.%
If needed, the latent states can be sampled using a simulation smoothing method discussed earlier.%
\footnote{In \Stan{}, the calculation of $p(y| .)$ is done in the \texttt{transformed parameter} or \texttt{model} blocks, while the sampling of the latent states is done in the \texttt{generated quantities} block.}
This two-step process is an example of partially collapsed Gibbs-sampling \parencite{VanDykPark2008a}.

To summarize, an efficient method to sample GDLMs in \Stan{} is:
\begin{enumerate}
\item Sample $\vartheta$ from $p(\vartheta | y)$ using HMC in \Stan{}.
  This requires integrating out the latent states, $\theta$, and calculating $p(y | \vartheta)$, which is done using a Kalman filter.
\item Sample $p(\theta | y, \vartheta)$ using a simulation smoother for a GDLM as in  \parencites{CarterKohn1994}{Fruehwirth-Schnatter1994}{DeJongShephard1995}{DurbinKoopman2002}[Ch 4.9]{DurbinKoopman2012}.
\end{enumerate}
I use this method to sample all the models in this work.
This required implementing the Kalman filter and simulation smoothing methods in \Stan{}.
I provide a full set of user-defined \Stan{} functions that implements Kalman filters, smoothers, and simulation sampling available at \textcite{Arnold2015c}.
Section \ref{sec:example-stan-program} provides the code for one of the \Stan{} modules used in this paper.%
\footnote{The full code for all models run in this work are available at \url{https://github.com/jrnold/dlm-shrinkage}.}

% This implementation of state space models in \Stan{} is one of the few state space specific samplers in general Bayesian programming language.
% Several \RLang{} packages implement Kalman filtering, smoothing, and simulation for Gaussian state space models; see \textcite{Tusell2011} for an overview.
% However, these do not provide a full Bayesian sampling solution, since the user would still need to write their own sampler for any other parameters going into the state space model, or if the state space model were a component of a more complex model.
% The  probabilistic programming language, \proglang{JAGS}, does not implement a specific sampler for Gaussian linear state space models.


\section{Example: Annual Nile River Flows}
\label{sec:nile}

A classic dataset that has been analyzed in many works and texts on times series and structural breaks is the annual flow volume of the Nile River between 1871 and 1970 \parencites{Cobb1978}{Balke1993}{DeJongPenzer1998}{}{DurbinKoopman2012}{CommandeurKoopmanOoms2011}.%
\footnote{The dataset is included with \RLang{} as \texttt{Nile} in the included package \pkg{datasets}.}
Figure \ref{fig:nile} plots the annual Nile river flow data.
This series seems to show a single large shift in the average level of the annual flow around 1899.
This level shift is attributed to construction of a dam at Aswan that started operation in 1902 or climate changes that reduced rainfall in the area \parencite[278]{Cobb1978}.
This example is used to see if the methods discussed earlier are able to recover this change point.

\begin{figure}
  \centering
  \includegraphics{\Sexpr{file.path(PROJECT_ROOT, "analysis", "figures", "nile-nile-1")}}
  \caption[Annual flow of the Nile River, \Sexpr{start(datasets::Nile)[1]}--\Sexpr{end(datasets::Nile)[1]}]{Annual flow of the Nile River, \Sexpr{start(datasets::Nile)[1]}--\Sexpr{end(datasets::Nile)[1]}. Previous work has found a break point in this series near 1899.}
  \label{fig:nile}
\end{figure}

I compare several models of this data.
In all models, $y_{t}$ is the annual flow of the Nile River,%
\footnote{Discharge at Aswan in $10^8 m^{3}$.} %
and consists of $n = \Sexpr{length(datasets::Nile)}$ annual observations from $\Sexpr{start(datasets::Nile)[1]}$ to $\Sexpr{end(datasets::Nile)[1]}$.
In all models the observations are normally distributed given the mean, $\mu_{t}$.%
\footnote{
  This may not be appropriate since that data contains possible outliers at 1879, 1913, and 1964.
  However, modeling outliers is not the purpose of this analysis, so I ignore that.
}
\begin{equation}
  \label{eq:10}
  \begin{aligned}[t]
    y_{t} &\sim \dnorm{\mu_{t}, \sigma^{2}}
  \end{aligned}
\end{equation}
The models differ in how they model the possibly time-varying mean, $\mu_{t}$.
I estimate the following models:
\begin{description}[font = \normalfont\ModelII]
\item[Constant] In this model, the mean is constant.
  \begin{equation}
    \label{eq:21}
    \begin{aligned}[t]
      \mu_{t} &= \mu_{0} && \text{for all $t$}
    \end{aligned}
  \end{equation}
\item[Intervention] This model includes an indicator variable for all years including and after 1899.
  This models a situation in which the researcher knows the change points in a series, and is manually accounting for them.
  \begin{equation}
    \mu_{t} = 
    \begin{cases}
      \mu_{0} & t < 1899 \\
      \mu_{0} + \omega & t \geq 1899
    \end{cases}
  \end{equation}
\item[Normal] In this model, the system errors are distributed normal.
  This corresponds to the typical local level model \parencites[Ch. 2]{DurbinKoopman2012}[Ch. 2]{WestHarrison1997}.%
  \footnote{For example, as implemented in the \RLang{} function \texttt{StructTS}.}
  \begin{equation}
    \label{eq:11}
    \begin{aligned}[t]
      \mu_{t} &= \mu_{t - 1} + \omega_{t} & \omega_{t} & \sim N(0, \tau^{2})
    \end{aligned}
  \end{equation}
\item[StudentT] In this model, the system errors are distributed Student $t$.
  \begin{equation}
    \label{eq:18}
    \begin{aligned}[t]
      \mu_{t} &= \mu_{t - 1} + \omega_{t} & \omega_{t} & \sim \dt{\nu}{0, \tau}
    \end{aligned}
  \end{equation}
  The prior distribution of the degrees of freedom parameter $\nu$ is that suggested by \textcites{JuarezSteel2010b}, a Gamma distribution with shape parameter 2 and rate parameter 0.1 (mode 10, mean 20, and variance 200), which places most of mass in the relevant region of the space.
\item[Laplace] In this model, the system errors are distributed Laplace (double exponential):
  \begin{equation}
    \label{eq:22}
    \begin{aligned}[t]
      \mu_{t} &= \mu_{t - 1} + \omega_{t} & \omega_{t} & \sim \dlaplace{0, \tau}
    \end{aligned}
  \end{equation}
\item[Horseshoe] In this model, the system errors are distributed horseshoe.
  \begin{equation}
    \label{eq:19}
    \begin{aligned}[t]
      \mu_{t} &= \mu_{t - 1} + \omega_{t} & \omega_{t} & \sim \dnorm{0, \tau^{2} \lambda^{2}_{t}} \\
      \lambda_{t} &\sim \dhalfcauchy{0, 1}
    \end{aligned}
  \end{equation}
\item[Horseshoe+] In this model, the system errors are distributed horseshoe.
  \begin{equation}
    \label{eq:23}
    \begin{aligned}[t]
      \mu_{t} &= \mu_{t - 1} + \omega_{t} & \omega_{t} & \sim \dnorm{0, \tau^{2} \lambda^{2}_{t}} \\
      \lambda_{t} &\sim \dhalfcauchy{0, \eta_{t}} \\
      \eta_{t} &\sim \dhalfcauchy{0, 1}
    \end{aligned}
  \end{equation}
\end{description}
For the global scale parameter, $\tau$, in \ModelII{Laplace}, \ModelII{StudentT}, \ModelII{Horseshoe}, and \ModelII{Horseshoe+}, I use the half-Cauchy prior $\tau \sim \dhalfcauchy{0, \frac{1}{n}}$.
In \ModelII{Normal} and \ModelII{Intervention}, $\tau$ is given a semi-informative half-Cauchy prior with a scale equal to a scalar multiple of the standard deviation of the data.

Figure \ref{fig:nile_mu_posterior} plots the posterior distribution of $\mu$ for each model.
The \ModelII{Normal} model does not show a clean break at 1899, instead it estimates a change occurring over several years. 
The \ModelII{Laplace} model looks similar to the \ModelII{Normal} model. 
While the Laplace distribution achieves sparsity in maximum likelihood estimates because because it places the posterior mode of parameters at zero, when the posterior mean is used as an estimate it does not. 
Since does not concentrate much mass near zero, it is not surprising that it does not perform much differently than the normal distribution \parencites{ParkCasella2008}.
Both the \ModelII{Horseshoe} and \ModelII{Horseshoe+} models produce a posterior distribution of $\mu$ that appears similar to the step function in the \ModelII{Intervention} model.
Additionally, the \ModelII{StudentT} also produces estimates similar to the \ModelII{Intervention}, but with slightly wider posterior distributions than the horseshoe models.
Figure \ref{fig:nile_mu_posterior} plots the posterior distribution of the system errors, $\omega$, for each model.
The \ModelII{Horseshoe}, \ModelII{Horseshoe+}, and \ModelII{StudentT} models all estimate $\E(\omega_{t} | y)$ near zero for all years but 1899.

Table \ref{tab:nile-model_comp} includes several model comparison statistics.
First, I compare the models on their fit to the observed data using the root mean squared error (RMSE) ($\mathrm{RMSE}(y)$), and the expected log predictive density calculated using the Widely Applicable Information Criterion (WAIC) ($\mathrm{elpd}_{WAIC}$) and importance sampled leave-one-out (LOO) cross-validation ($\mathrm{elpd}_{loo}$).
The RMSE is defined as $\sqrt{\frac{1}{n} \sum_{i} {(y_{i} - \E(\mu_{i} | y))}^{2}}$.
The expected log probability density statistics are measures of the model's fit to an new out-of-sample observation.
The log probability density of a new observation is the expected value of the posterior density of a future observation, $\log p(\tilde{y} | \theta, y)$, where $\tilde{y}$ is the value of a future observation, and $\theta$ includes all parameters in the model.
Since the value of the future observation, $\tilde{y}$, is unknown, the expected log probability density averages over the predictive distribution of $\tilde{y}$, $\mathrm{elpd} = \E_{f}(\log p(\tilde{y} | \theta, y_{i}))$. 
However, the distribution of future values is in general also unknown, which is where the two approximations come it. 
$\mathrm{elpd}_{WAIC}$ approximates elpd using an information criteria similar to AIC, BIC or DIC, taking the in-sample log-likelihood and penalizing it for model complexity.
$\mathrm{elpd}_{loo}$ approximates elpd using leave-one-out cross validation.
See \textcite{GelmanCarlinSternEtAl2013a}, \textcites{GelmanVehtari2014a}, or \textcites{GelmanHwangVehtari2014a} for more thorough discussion of elpd and predictive measures for Bayesian model comparison.%
\footnote{
  The way these are calculated does not fully account for the time-series nature of this data.
  The measures presented here should be seen as approximating the fit of the model to a previously missing value within the time-series.
  To calculate $\mathtt{elpd}_{WAIC}$ and $\mathtt{elpd}_{loo}$, I use the \textbf{loo} \RLang{} package, which implements the methods described in \textcite{GelmanVehtari2014a}.
}
In the RMSE, a lower value indicates a better fit, for elpd, a higher value indicates is a better fit.
Second, I compare the fits of the model to the ``true'' values of $\mu_{t}$. 
But, since this is real data, I do not know the true values of $\mu_{t}$.
Instead, I will will compare the other models to posterior mean estimate of \ModelII{Intervention} model. 
The column $\textrm{RMSE}(\mu)$ of Table \ref{tab:nile-model_comp} is the root mean squared error of the models compared to the posterior mean of $\mu$ as estimated by the \ModelII{Intervention} model, defined as $\sqrt{\frac{1}{n} \sum (\E(\mu_{t}| y) - \bar{\mu}_{t})^{2}}$, where $\bar{\mu}_{t}$ is the posterior mean of $\mu_{t}$ in the \ModelII{Intervention} model.
As with comparisons of fit to the observed data, the \ModelII{Horseshoe} and \ModelII{Horseshoe+} models have the lowest RMSE, although the \ModelII{StudentT} model is close.


\begin{figure}[htpb!]
  \centering
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[]{\Sexpr{file.path(fig_path, "nile-m0_mu-1.pdf")}}
    \caption{\ModelII{Constant}}
  \end{subfigure}%
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[]{\Sexpr{file.path(fig_path, "nile-m1_mu-1.pdf")}}
    \caption{\ModelII{Intervention}}
  \end{subfigure}
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[]{\Sexpr{file.path(fig_path, "nile-m2_mu-1.pdf")}}    
    \caption{\ModelII{Normal}}
  \end{subfigure}%
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[]{\Sexpr{file.path(fig_path, "nile-m3_mu-1.pdf")}}
    \caption{\ModelII{StudentT}}
  \end{subfigure}
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[]{\Sexpr{file.path(fig_path, "nile-m4_mu-1.pdf")}}    
    \caption{\ModelII{Laplace}}
  \end{subfigure}%
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[]{\Sexpr{file.path(fig_path, "nile-m5_mu-1.pdf")}}
    \caption{\ModelII{Horseshoe}}
  \end{subfigure}
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[]{\Sexpr{file.path(fig_path, "nile-m6_mu-1.pdf")}}    
    \caption{\ModelII{Horseshoe+}}
  \end{subfigure}
  \caption[Posterior distributions of $\mu_t$ for models of the Nile River annual flow data.]{Posterior distributions of $\mu_t$ for models of the Nile River annual flow data. The line is the posterior mean; the range of the ribbon the 2.5--97.5\% percentiles of the posterior distribution.}
  \label{fig:nile_mu_posterior}
\end{figure}


\begin{figure}[htpb!]
  \centering
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[]{\Sexpr{file.path(fig_path, "nile-m2_omega-1.pdf")}}    
    \caption{\ModelII{Normal}}
  \end{subfigure}%
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[]{\Sexpr{file.path(fig_path, "nile-m3_omega-1.pdf")}}
    \caption{\ModelII{StudentT}}
  \end{subfigure}
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[]{\Sexpr{file.path(fig_path, "nile-m4_omega-1.pdf")}}    
    \caption{\ModelII{Laplace}}
  \end{subfigure}%
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[]{\Sexpr{file.path(fig_path, "nile-m5_omega-1.pdf")}}
    \caption{\ModelII{Horseshoe}}
  \end{subfigure}
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[]{\Sexpr{file.path(fig_path, "nile-m6_omega-1.pdf")}}    
    \caption{\ModelII{Horseshoe+}}
  \end{subfigure}
  \caption[Posterior distributions of $\omega_t$ for models of the Nile River annual flow data]{Posterior distributions of $\omega_t$ for models of the Nile River annual flow data. The point is the posterior mean; the range of the line is the 2.5--97.5\% percentiles of the posterior distribution.}
  \label{fig:nile_omega_posterior}
\end{figure}



\begin{table}[thbp]
  \centering
  \input{\Sexpr{file.path(tab_path, "nile-tab_model_comp.tex")}}
  \caption{Model comparison statistics for models of the Nile Rive annual flow data.}
  \label{tab:nile-model_comp}
\end{table}


\section{Change Points in Levels and Trends}
\label{sec:linear-filtering}


The use of sparsity inducing priors can be extended to model change points in trends in addition to the level.%
\footnote{\textcite{KimKohBoydEtAl2009} and \textcite{Tibshirani2014} consider similar problems in a maximum likelihood framework with $\ell_{1}$ regularization.}
The local level model considered in section \ref{sec:chang-as-vari} can be extended to a local trend model \parencites[Ch 3.2]{DurbinKoopman2012}[Ch 7]{WestHarrison1997},
\begin{equation}
  \label{eq:20}
  \begin{aligned}[t]
    y_{t} &= \mu_{t} + \epsilon_{t} & \epsilon_{t} & \sim \dnorm{0, \sigma^{2}} \\
    \mu_{t} &= \mu_{t - 1} + \alpha_{t - 1} + \omega_{1, t} \\
    \alpha_{t} &= \alpha_{t - 1} + \omega_{2, t} 
  \end{aligned}
\end{equation}
In Equation (\ref{eq:20}), there are two states: $\mu_{t}$ is the current level, and $\alpha_{t}$ is the current trend (change in the level).
The level is changing over time both due to the current value of the trend, $\alpha_{t}$, and errors, $\omega_{1,t}$.
While the trend is changing over time due to error, $\omega_{2,t}$.
This model will allow for change points in both the level and trend if sparsity inducing shrinkage priors are used for $\omega_{1}$ and $\omega_{2}$.
The system errors in a local trend model could be modeled with an arbitrary covariance structure, but in this work, I follow the suggestion of \parencite[Ch 7.]{WestHarrison1997},
\begin{equation}
  \label{eq:29}
  \begin{aligned}[t]
  \begin{bmatrix}
    \omega_{1, t} \\
    \omega_{2, t}
  \end{bmatrix} 
  & '\sim \dnorm{0,
    L \diag(W_{1}^{2}, W_{2}^{2}) L'
  } ; &
  L &= 
  \begin{bmatrix}
    1 & 1 \\
    0 & 1 
  \end{bmatrix}
  \end{aligned}
\end{equation}
Since the sparsity inducing shrinkage priors discussed can be represented as scale-mixtures of normals, $\omega$, is distributed as:
\begin{equation}
\label{eq:26}
\begin{bmatrix}
  \omega_{1, t} \\
  \omega_{2, t}
\end{bmatrix}
\sim \dnorm{0,
  \begin{bmatrix}
    \tau_{1}^{2} \lambda_{1, t}^{2} + \tau_{2} \lambda_{2, t}^{2} & \tau_{2} \lambda_{2, t} \\
    \tau_{2}^{2} \lambda_{2, t} & \tau_{2}^{2} \lambda_{2, t}
  \end{bmatrix}
}
\end{equation}



\section{Example: George W. Bush Approval Ratings}
\label{sec:george-w.-bush}

\textcite{RatkovicEng2010} use the approval ratings for George W. Bush, displayed in Figure \ref{fig:bush_approval}.
George W. Bush's approval ratings are difficult to fit with a typical smoothing methods because it was subject to two large jumps in his approval ratings: September 11th, 2001 and at the start of the Iraq War on March 20, 2003.
The data used in this example consist of \Sexpr{nrow(BushApproval)} polls between \Sexpr{date_format(min(BushApproval[["start_date"]]))} and \Sexpr{date_format(min(BushApproval[["start_date"]]))} from the Roper Center Public Opinion \footnote{From \url{http://webapps.ropercenter.uconn.edu/CFIDE/roper/presidential/webroot/presidential_rating_detail.cfm?allRate=True\&presidentName=Bush\#.UbeB8HUbyv8}.}

\begin{equation}
  \label{eq:30}
  \begin{aligned}[t]
    y_{t} &= \mu_{t} + \epsilon_{t} & \epsilon_{t} & \sim \dnorm{0, \sigma^{2}} \\
    \mu_{t} &= \alpha_{t} +  \mu_{t - 1} + \partial \mu_{t - 1} + \omega_{1, t} \\
    \partial \mu_{t} &= + \partial \mu_{t - 1} + \omega_{2, t} \\
    \begin{bmatrix}
      \omega_{1, t} \\
      \omega_{2, t}
    \end{bmatrix} &
                    \sim \dnorm{0,
                    \begin{bmatrix}
                      \tau_{1}^{2} \lambda_{1, t}^{2} + \tau_{2} \lambda_{2, t}^{2} & \tau_{2} \lambda_{2, t} \\
                      \tau_{2}^{2} \lambda_{2, t} & \tau_{2}^{2} \lambda_{2, t}
                    \end{bmatrix}
                    }
  \end{aligned}
\end{equation}


\begin{description}[font = \normalfont\ModelII]
\item[Normal]
  System errors are distributed normal.
  $\lambda_{i, t} = 1$  for all $i$ for all $t$, $\alpha_{t} = 0$ for all $t$.
\item[Intervention]
  System errors are distributed normal, $\lambda_{i, t} = 1$  for all $i$ for all $t$.
  There are manual interventions after 9/11 and the the Iraq War. 
  The values of $\alpha$ for those two dates are non-zero and estimated, all other $\alpha_{t}$ are zero.
  This corresponds to a manual intervention for known change points.
\item[Horseshoe] The system errors are distributed horseshoe. $\alpha_{t} = 0$ for all $t$, and 
  \begin{equation}
    \label{eq:27}
    \begin{aligned}[t]
    \lambda_{i, t} & \sim \dhalfcauchy{0, 1} & \text{for $i \in 1, 2$} \\
    \tau_{i} & \sim \dcauchy{0, \frac{1}{n}}
    \end{aligned}
  \end{equation}
\item[Horseshoe+] The system errors are distributed horseshoe+ $\alpha_{t}= 0$ for all $t$, and
  \begin{equation}
    \label{eq:25}
    \begin{aligned}
    \lambda_{i, t} & \sim \dhalfcauchy{0, \eta_{i, t}} &&\text{  for $i \in 1, 2$.} \\
    \eta_{i, t} & \sim \dhalfcauchy{0, 1} \\
    \tau_{i} & \sim \dcauchy{0, \frac{1}{n}}
    \end{aligned}
  \end{equation}
\end{description}


Figures \ref{fig:bush_mu1} and \ref{fig:bush_mu2} plot the posterior distribution of $\mu_{t}$ for the models.
The \ModelII{Normal} model shows Bush's approval rating rising before 9/11 and shows a lot of small variation.
The \ModelII{Horseshoe} and \ModelII{Horseshoe+} models more closely resemble the \ModelII{Intervention} model:
Bush's approval rating are loping downward or steady until 9/11, and the approval ranting is mostly smooth apart from those sharp jumps.
Table \ref{tab:bush_model_comp} shows the RMSE and expected log predictive densities of the these models. 
The horseshoe models fit the the data better than a normal distribution, but less well than the \ModelII{Intervention} model.

\begin{figure}[thbp!]
  \centering
  \includegraphics{\Sexpr{file.path(fig_path, "bush-approval-1")}}
  \caption{Approval ratings of President George W. Bush}
  \label{fig:bush_approval}
\end{figure}

\begin{figure}[thbp!]
  \centering
  \begin{subfigure}[b]{\linewidth}
    \includegraphics[]{\Sexpr{file.path(fig_path, "bush-m1_mu-1.pdf")}}
    \caption{\ModelII{Normal}}
  \end{subfigure}

  \begin{subfigure}[b]{\linewidth}
    \includegraphics[]{\Sexpr{file.path(fig_path, "bush-m3_mu-1.pdf")}}
    \caption{\ModelII{Intervention}}
  \end{subfigure}
  \caption{Posterior distribution of $\mu$ for \ModelII{Normal} and \ModelII{Intervention} models.}
  \label{fig:bush_mu1}
\end{figure}

\begin{figure}[thbp!]
  \begin{subfigure}[b]{\linewidth}
    \includegraphics[]{\Sexpr{file.path(fig_path, "bush-m2_mu-1.pdf")}}
    \caption{\ModelII{Horseshoe}}
  \end{subfigure}

  \begin{subfigure}[b]{\linewidth}
    \includegraphics[]{\Sexpr{file.path(fig_path, "bush-m4_mu-1.pdf")}}
    \caption{\ModelII{Horseshoe+}}
  \end{subfigure}
  \caption{Posterior distribution of $\mu$ for \ModelII{Horseshoe} and \ModelII{Horseshoe+} models.}
  \label{fig:bush_mu2}
\end{figure}


\begin{table}[thbp!]
  \centering
  \input{\Sexpr{file.path(tab_path, "bush-tab_model_comp.tex")}}
  \caption{}
  \label{tab:bush_model_comp}
\end{table}



\section{Discussion}
\label{sec:discussion}




\section{Example Stan Program}
\label{sec:example-stan-program}

Example of the code to estimate the Horseshoe change point model.
The DLM related user-defined functions in the \texttt{functions} block are excluded.


\inputminted[firstline=5,style=bw]{stan}{\Sexpr{file.path(PROJECT_ROOT, "stan", "changepoint_horseshoe.stan.mustache")}}  


%  LocalWords:  iid Efron GiordaniKohn2008 Tibshirani1996 Tipping2001
%  LocalWords:  PolsonScott2010 ParkCasella2008 Hans2009 DLMs Kalman
%  LocalWords:  CaronDoucet2008 BrownGriffin2010 FFBS Tusell2011 HMC
%  LocalWords:  CarvalhoPolsonScott2010 DurbinKoopmans2012 TVP Polson
%  LocalWords:  ReisSalazarGamerman2006 Ratkovic Carvalho SDDLM XXXX
%  LocalWords:  ARIMA GDLM GDLMs CalderiaZorn1998 WesternKleykamp2004
%  LocalWords:  Spirling2007a Spirling2007b Park2010 Park2011 OECD 's
%  LocalWords:  Blackwell2012 MitchellBeauchamp1988a Efron2008a rstan
%  LocalWords:  Graybacks Page1954a Hinkley1970a BaiPerron2003a XXX
%  LocalWords:  OlshenVenkatramanLucitoEtAl2004 BaiPerron1998 Yao1984
%  LocalWords:  KillickFearnheadEckley2012 BarryHartigan1993 Chib1998
%  LocalWords:  Fearnhead2006a FearnheadLiu2007a TibshiraniEtAl2005
%  LocalWords:  HarchaouiLevy Leduc2010 ChanYauZhang2014 Balke1993
%  LocalWords:  BhadraDattaPolsonEtAl2015a CarvalhoPolsonScott2009
%  LocalWords:  DattaGhosh2012 PasKleijnVaart2014a DeJongPenzer1998
%  LocalWords:  PetrisPetroneEtAl2009 DurbinKoopman2012 Cobb1978 RMSE
%  LocalWords:  CommandeurKoopmanOoms2011 StudentT JuarezSteel2010b
%  LocalWords:  WestHarrison1997 ShumwayStoffer2010 SARIMA Harvey1989
%  LocalWords:  CarterKohn1994 Fruehwirth Schnatter1994 paremeters
%  LocalWords:  DeJongShephard1995 DurbinKoopman2002 Stan2015a WAIC
%  LocalWords:  CarpenterGelmanHoffmanEtAl2015a HoffmanGelman2014a
%  LocalWords:  VanDykPark2008a LOO Tibshirani2014 KimKohBoydEtAl2009
%  LocalWords:  RatkovicEng2010 nrow BushApproval DLM stan StructTS
%  LocalWords:  elpd Buethe2002a Lieberman2002a Pierson2004 Beck1989
%  LocalWords:  Jackman2009 PolsonScott2012 Harvey1990 Arnold2015c
%  LocalWords:  AIC BIC GelmanCarlinSternEtAl2013a GelmanVehtari2014a
%  LocalWords:  GelmanHwangVehtari2014a loo
