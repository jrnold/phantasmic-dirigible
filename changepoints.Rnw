
\section{Introduction}
\label{sec:introduction}

This paper 


\begin{itemize}
\item How change-points are typically 
\item Representation as a variable selection problem
\item Discussion of shrinkage priors
\item Dynamic linear model representation
\end{itemize}

Political and social processes are rarely, if ever, constant over time.
Thus, political scientists often have a need to estimate time-varying parameter (TVP) models.
There exist two broad approaches to estimating time-varying parameters: structural break approaches, which include dummy variables and change point models, and smoothing approaches, which include dynamic linear models and smoothing splines. 
Both approaches estimate TVP when the number of observations is small relative to the number of time periods. 
Smoothing approaches keep the parameter differences small in all periods, while structural break approaches restrict parameter changes to a few locations. 
These two approaches are often viewed as distinct and estimated using different methods, forcing the researcher to choose between which model to use.
Many processes the changes in the process are characterized by many periods of stability and a few periods of possibly rapid and large, change (Ratkovic and Eng 2010).
Historically, existing smoothing methods have had a difficulty estimating these sorts of processes, so structural break models have been used

This paper presents a simple and extensible method to estimate time-varying parameters that may be subject to structural breaks. 
Time-varying parameters with possible structural breaks can be estimated within a continuous state-space model, and in particular as a dynamic linear model, by placing a shrinkage prior on the distribution of the state disturbances.
The intuition behind this can be illustrated with a simple model.
Suppose there is a vector of observed data, $y_{1}, \dots, y_{n}$, drawn from a normal distribution with a time varying mean, $\mu_{1}, \dots, \mu_{n}$.
This can be represented as a dynamic linear model,

The difference between "structural breaks" and "smoothing" data-generating processes and estimation techniques is whether the XXXX vector is sparse (most XXXX = 0) or dense (most XXXX neq 0).
A common model is to apply a normal distribution to XXXX
A normal XXXX  estimate change over time, but it cannot produce sparse estimates of XXXX.
The thin tails of the normal distribution will tend to over-smooth periods around structural breaks, under-smooth periods in which there were no structural breaks.
However, estimating sparse parameter vectors is a general problem that has received considerable attention lately in large-p, small-n problems (Tibshirani1996 1996; Polson and Scott 2010). This paper applies some of the advances in sparse parameter estimation to estimating time-varying parameters with structural breaks. Structural breaks, i.e. sparse, can be estimated by placing a shrinkage prior on.
While there are a large number of Bayesian shrinkage priors proposed, this paper will use the Horseshoe prior (HS) distribution introduced in Carvalho, Polson, and Scott (2009) and Carvalho, Polson, and Scott (2010). I will refer to a dynamic linear models with shrinkage priors on the state disturbances as a sparse disturbance dynamic linear models (SDDLM).

This method does not require specifying the number of structural breaks ex ante.
\begin{enumerate}
\item The sparsity of XXXX will determine the number of structural breaks, and this sparsity can be estimated from the data.
Structural breaks can be detected from the posterior distribution of XXX using several rules. Not only do
the number of structural breaks need not be specifieded beforehand, this method will work reasonable
well even if the underlying data-generating process is dense, \ie{}the parameter changes in each period.
\item 2. This method is extensible. 
  Dynamic linear models incorporate a wide variety of models, including ARIMA
  and structural time-series, cubic splines, and regressions with time-varying coefficients. Any model in
  which the parameter of interest can be expressed as a latent state in a dynamic linear model can be
  altered to assign a shrinkage prior to the state disturbance in order to make it robust to or to detect
  structural breaks for that parameter. While many structural break methods are specific to single
  models, this paper shows how sparse-disturbance DLMs can be estimated within the general purpose
  Bayesian software, \Stan{}.
\item The method allows for easy estimation of structural breaks in multiple parameters which can be either
independent or correlated. Estimating independent structural breaks in a discrete state space model,
like change-point models, results in the state-space multiplying exponentially.
assigned a shrinkage prior.
\item This method is efficient in both programmer and computational time, while still retaining the extensibility
to estimate a wide variety of models. Since most shrinkage priors, including the HS distribution
used in the paper, are scale-mixture of normal distributions, this state space model estimated is a
(conditionally) Gaussian dynamic linear model (GDLM). There are can take advantage of the computationally
efficient methods of mode finding and sampling from GDLMs, such as the Kalman filter and
Forward-Filter Backwards-Sample. This paper shows how a combination of Stan, a general purpose
Bayesian software program, and R can be used to easily estimate and sample from the posterior of
dynamic linear models.
\end{enumerate}

Political science paper on change-points \parencites{CalderiaZorn1998}{WesternKleykamp2004}{Spirling2007a}{Spirling2007b}{Park2010}{Park2011}{Blackwell2012}.

\section{Change points as a Variable Selection Problem}
\label{sec:chang-as-vari}

For simplicity, I start with the case of change-points in the mean, and later generalize to other cases.
There are $n$ observations, $y_{1}, \dots, y_{n}$, with a time-varying mean, $\mu_{t}$, that takes $M$ values, $\mu^{*}_{1}, \dots, \mu^{*}_{M}$,  with change-point locations, $\tau_{1}, \dots, \tau_{m}$,
\begin{equation}
  \label{eq:1}
  \begin{aligned}[t]
    y_{t} & = \mu_{t} + \epsilon_{t} && \text{$\epsilon_{t}$ are iid with $\E(\epsilon) = 0$.} \\
    \mu_{t} &= \mu^{*}_{m} && \text{if $\tau_{m} \leq t \leq \tau_{m + 1}$, $\mu_{1} = \mu^{*}_{1}$, $\mu_{n} = \mu^{*}_{M}$.}
  \end{aligned}
\end{equation}
There are a variety of change-point approaches to this problem in both classical \parencites{Page1954a}{Hinkley1970a}{BaiPerron2003a}{OlshenVenkatramanLucitoEtAl2004}{BaiPerron1998}{KillickFearnheadEckley2012} and Bayesian statistics \parencites{Yao1984}{BarryHartigan1993}{Chib1998}{Fearnhead2006a}{FearnheadLiu2007a}.


An alternative approach is to rewrite the problem in equation (\ref{eq:1}) to focus on the values of the changes in the mean (innovations), $\omega_{t} = \mu_{t} - \mu_{t - 1}$, rather than the locations of the change points:
\begin{equation}
  \label{eq:2}
  \begin{aligned}[t]
    y_{t} &= \mu_{t} + \epsilon_{t} && \text{$\epsilon_{t}$ are iid with $\E(\epsilon) = 0$.} \\
    \mu_{t} &= \mu_{t - 1} + \omega_{t}
  \end{aligned}
\end{equation}
In a change points situation, the innovations $\omega_{t}$ are sparse, meaning that most values of $\omega_{t}$ are zero, and only a few are non-zero.
The times of the change points are not directly estimated, but are those times at which $\omega_{t}$ are non-zero.
This turns the problem from one of segmentation, to one of variable selection, in which the goal is to find the non-zero values of $\omega_{t}$ and estimate their values.
\footnote{In Equation (\ref{eq:2})The initial value of $\mu_{1}$, or equivalently $\mu_{0}$, needs to be given a value.
%This will likley work better with the formulation that initilizes at t = 1, since we should assume \omega_1=0.
}
The natural approach to this variable selection problem is to explicitly model the values of $\omega$ as a discrete mixture between a point mass at zero for the non-change points, and a heavy-tailed alternative \textcite{MitchellBeauchamp1988a}{Efron2008a}:
\begin{equation}
  \label{eq:3}
  \omega_{t} = \pi g(\omega) + (1 - \pi) \delta_{0}
\end{equation}
where $g$ is some heavy-tailed distribution.
Equation (\ref{eq:3}) is often called a ``spike and slab'' prior \textcite{MitchellBeauchamp1988a}.
Since it explicitly models the two groups (zeros and non-zeros), \textcite{Efron2008a} calls this the two-group answer to the two-groups problem.
This prior is convenient because it directly provides for each time a posterior probability that it is a change point (non-zero).
\textcite{GiordaniKohn2008} propose using the representation in Equation (\ref{eq:2}) with the a discrete mixture distribution for $\omega$, as in Equation (\ref{eq:3}), to estimate change points.

Recent work in Bayesian computation has focused on one-group solutions to the variable selection problem, which combine shrinkage (regularization) and variable selection through the use of continuous distribution with a large spike at zero and wide tails \parencite{}
This is analogous to the sparse regularization literature in frequentist statistics spawned by LASSO \parencite{Tibshirani1996} and its the numerous variations.
In this approach, several papers have proposed using LASSO-type penalties in a maximum likelihood approach to estimate change-points \parencites{TibshiraniEtAl2005}{HarchaouiLevy-Leduc2010}{ChanYauZhang2014}.
In the Bayesian literature or shrinkage in sparse situations numerous distributions have been proposed. 
This paper will look at four of them: the Student's $t$ distribution \parencite{Tipping2001}, the Double Exponential or Laplace \parencite{ParkCasella2008}{Hans2009}, the Horseshoe \parencite{CarvalhoPolsonScott2010}, and Horseshoe+ \parencite{BhadraDattaPolsonEtAl2015a}.


\begin{description}

\item[Student's $t$] The Student's $t$ distribution for $x \in \R$  with scale $\tau \in \Rp$ and degrees of freedom $\nu \in \Rp$,
\begin{equation}
  \label{eq:6}
  p(\omega_{t} | \tau, \nu) = \frac{\Gamma\left(\frac{\nu + 1}{2}\right)}{\sqrt{\nu \pi} \Gamma\left( \frac{\nu}{2} \right)} \left( 1 + \frac{\omega_{t}^{2}}{\nu} \right)
\end{equation}
The Student's $t$ distribution can also be expressed as a scale mixture of normals,%
\footnote{
  $\dinvgamma{x | \alpha, \beta}$ is an inverse-gamma distribution for $x \in \Rp$ with shape $\alpha \in \Rp$ and inverse-scale $\beta \in \Rp$,
  \begin{equation}
    \label{eq:9}
    \dinvgamma{x | \alpha, \beta} = \frac{\beta^{\alpha}}{\Gamma(\alpha)} x^{-(\alpha - 1)} \exp 
    \left(
      \beta \frac{1}{x}
    \right)
  \end{equation}
}
\begin{equation}
  \label{eq:5}
  \begin{aligned}[t]
  \omega_{t} | \tau, \nu, \lambda_{t} &\sim \dnorm{0, \tau^{2} \lambda_{t}^{2}} \\
  \lambda_{t}^{2} | \nu & \sim \dinvgamma{\frac{\nu}{2}, \frac{\nu}{2}}
  \end{aligned}
\end{equation}
\parencite{Tipping2001} uses the Student's $t$ for sparse shrinkage by letting the degrees of freedom $\nu \to 0$.

\item[Double Exponential] The double exponential (Laplace) with scale $\tau$,
\begin{equation}
  \label{eq:14}
  p(\omega_{t}| \tau) = \frac{1}{2 \tau} \exp 
  \left(
    - \frac{\abs{\omega_{t}}}{\tau}
  \right)
\end{equation}
The double exponential distribution can also be expressed as a scale-mixture of normal distributions,%
\footnote{
  $\dexp{x | \beta}$ is the exponential distribution for $x \in \Rp$ with inverse-scale (rate) parameter $\beta \in \Rp$,
    \begin{equation}
      \label{eq:13}
      \dexp{x | \beta} = \beta \exp(- \beta x)
    \end{equation}
}
\begin{equation}
  \label{eq:12}
  \begin{aligned}[t]
  \omega_{t} | \tau, \lambda_{t} &\sim \dnorm{0, \tau^{2} \lambda_{t}^{2}} \\
  \lambda_{t}^{2} & \sim \dexp{\frac{1}{2}}
  \end{aligned}
\end{equation}
The double exponential distribution is the distribution that corresponds to the $\ell_{1}$ penalty used in LASSO \parencites{ParkCasella2008}{Hans2009}.
The LASSO achieves sparsity due to the spike in the double exponential distribution at zero places also places the mode of estimates at zero. 
However, the although the double exponential distribution has a spike at zero, it does not produce sparse posterior means since it has relatively little mass close to zero.
Additionally, the tails of the double exponential are narrow, so it tends to excessively shrink large signals \parencites{CarvalhoPolsonScott2010}.

\item[Horseshoe] The horseshoe distribution \parencite{CarvalhoPolsonScott2009}{CarvalhoPolsonScott2010} is defined hierarchically as a scale-mixture of normals
\begin{equation}
  \label{eq:7}
  \begin{aligned}[t]
    \omega_{t} | \lambda_{t}, \tau & \sim \dnorm{0, \tau^{2} \lambda_{t}^{2}} \\
    \lambda_{t}  & \sim \dhalfcauchy{0, 1}
  \end{aligned}
\end{equation}
where $\dhalfcauchy{x | 0, s}$ denotes half-Cauchy distribution with a scale parameter $s$, and density
\begin{equation}
  \label{eq:8}
  p(x | s) = \frac{2}{\pi x \left(1 + {\left(\frac{x}{s}\right)}^{2}\right)}
\end{equation}
The Horseshoe distribution has some theoretically attractive properties for shrinkage and variable selection \parencites{CarvalhoPolsonScott2009}{CarvalhoPolsonScott2010}{DattaGhosh2012}{PasKleijnVaart2014a}.

\item[Horseshoe+] The horseshoe+ distribution \textcite{BhadraDattaPolsonEtAl2015a} is similar to the Horseshoe distribution, but with an additional term,
\begin{equation}
  \label{eq:15}
  \begin{aligned}[t]
    \omega_{t} | \lambda_{t}, \eta_{t}, \tau & \sim \dnorm{0, \tau^{2} \lambda_{t}^{2}} \\
    \lambda_{t}  & \sim \dhalfcauchy{0, \eta_{t}} \\
    \eta_{t} & \sim \dhalfcauchy{0, 1} 
  \end{aligned}
\end{equation}
\end{description}

The shrinkage distributions considered here also global-local scale mixtures of normals, with a global variance component $\tau$, and local variance components $\lambda_{t}$ \parencite{PolsonScott2010}.
The global variance component $\tau$ concentrates the distribution around zero, while the local variance components allow individual values to be far from zero.

To recap, the propsed model for changepoints is
\begin{align}
  \label{eq:16}
  y_{t} & \sim \epsilon_{t} & \text{$\epsilon_{t}$ iid, $\E(\epsilon) = 0$, $\Var(\epsilon) = \sigma^{2}$} \\
  \mu_{t} &= \mu_{t - 1} + \sigma \omega_{t}
\end{align}
where $\omega_{t}$ is given a shrinkage prior distribution that induces sparsity.
This work will propose horseshoe and horseshoe+ distributions as those shrinkage priors, and compare them other sthrinkage priors such as the Student's $t$ and double exponential distributions.

While this one-group approach does not provide as clear an estimate of the locations or existence of change points, it is useful for several reasons.
\begin{enumerate}
\item For many time-varying parameter processes typically modeled with a change-point model, the situation akin to a situation in which the parameter is changing in all periods, but most of these changes are small relative to that of a few periods.
  In other words, the innovations $\omega$ are never zero; it is just that in most periods they are relatively small.
  This seems plausible for many processes modeled by political scientists in which there is no physical reason to think there are discrete states, apart from perhaps institutional changes.
  It may be useful for the researcher to model it with a step function (change points) for parsimony and interpretation, but the data-generating process is likely closer to one in which things are usually changing a little, but in some periods they change a lot. 
  I would argue that the data generating processes considered by political science papers using change points falls into this category, for example Supreme court dissent and consensus \parencite{CalderiaZorn1998}, wage growth in OECD states \parencite{WesternKleykamp2004}, casualties in the Iraq War \parencite{Spirling2007a}, presidential use of force \parencite{Park2010}, and campaign contributions \parencite{Blackwell2012}.
\item When the goal is estimation or prediction of the parameters, rather than the locations of the change-points, then the changes in the parameter is continuous anyways after integrating over the posterior distribution of the change-points. 
  The one-group approach approximates this distribution.
\end{enumerate}

\section{Estimation}
\label{sec:estimation}



\subsection{Dynamic Linear Models}
\label{sec:dynam-line-models}

A dynamic linear model, also called Gaussian state space models, is represented by a system of equations
\begin{equation}
  \label{eq:4}
  \begin{aligned}[t]
  y_{t} &\sim \dnorm{b_{t} + F_{t} \theta_{t}, V_{t}} \\
  \theta_{t} &\sim \dnorm{g_{t} + G_{t} \theta_{t - 1}, W_{t}} 
  \end{aligned}
\end{equation}
This is covered in INSERT TEXTS.

The model in Section \Stan{}, a probabilistic programming language, through the \RLang{} package \textbf{rstan}.
These models could be estimated in \Stan{} directly by translating the models described in the previous section into a \Stan{} model.
Although the Horseshoe and Horseshoe+ distributions are not directly implemented in \Stan{}, they are easily implemented in Stan through their hierarchical representation.%
\footnote{See the examples in \url{http://andrewgelman.com/2015/02/17/bayesian-survival-analysis-horseshoe-priors/} and \url{http://becs.aalto.fi/en/research/bayes/diabcvd/}.}

However, Gaussian state space models can be estimated more efficiently by marginalizing over the latent states while sampling other parameters, and then sampling the latent states using a Forward-Filter Backwards Sampler (FFBS) type.

Several \RLang{} packages implement Kalman filtering, smoothing, and simulation for Gaussian state space models; see \textcite{Tusell2011} for an overview.
However, these do not provide a full Bayesian sampling solution, since the user would still need to write their own sampler for any other parameters going into the state space model, or if the state space model were a component of a more complex model.
The  probabilistic programming language, \proglang{JAGS}, does not implement a specific sampler for Gaussian linear state space models.

Let $\vartheta$ refer to all parameters in the system vectors and matrices in the state space models,
\begin{enumerate}
\item Sample $\vartheta$ from $p(\vartheta | y)$ using HMC in \Stan{}.
  This requires integrating out the latent states, $\theta$, and calculating $p(y | \vartheta)$.
  The value of $p(y | \vartheta)$ is calculated using the Kalman filter.
\item Sample $p(\theta | y, \vartheta)$ using a simulation sampler for the linear Gaussian state space model.
\end{enumerate}
I use this method for all models in the paper. 
To implement this method in \Stan{}.
I provide a full set of user-defined \Stan{} functions that implements Kalman filters, smoothers, and simulation sampling.
Section XXXX provides documentation of these functions.



\section{Change Point Examples}
\label{sec:examples}


\begin{itemize}
\item Blocks
\item Nile
\item Civil War Greenbacks / Graybacks
\end{itemize}






\section{Efficient Estimation of DLMs in Stan}
\label{sec:effic-estim-dlms}

Comparison of Kalman filter FFBS implementations: @DurbinKoopmans2012
Other comparisons of sampling schemes: @ReisSalazarGamerman2006.
Comparison of speed of implementations in R, focusing on mode finding: @Tusell2011.


Marginalizing over latent states is necessary in discrete state space models in \Stan{} since HMC does cannot sample discrete parameters CITE.


%  LocalWords:  iid Efron GiordaniKohn2008 Tibshirani1996 Tipping2001
%  LocalWords:  PolsonScott2010 ParkCasella2008 Hans2009 DLMs Kalman
%  LocalWords:  CaronDoucet2008 BrownGriffin2010 FFBS Tusell2011 HMC
%  LocalWords:  CarvalhoPolsonScott2010 DurbinKoopmans2012 TVP Polson
%  LocalWords:  ReisSalazarGamerman2006 Ratkovic Carvalho SDDLM XXXX
%  LocalWords:  ARIMA GDLM GDLMs CalderiaZorn1998 WesternKleykamp2004
%  LocalWords:  Spirling2007a Spirling2007b Park2010 Park2011 OECD
%  LocalWords:  Blackwell2012 MitchellBeauchamp1988a Efron2008a rstan
%  LocalWords:  Graybacks Page1954a Hinkley1970a BaiPerron2003a
%  LocalWords:  OlshenVenkatramanLucitoEtAl2004 BaiPerron1998 Yao1984
%  LocalWords:  KillickFearnheadEckley2012 BarryHartigan1993 Chib1998
%  LocalWords:  Fearnhead2006a FearnheadLiu2007a TibshiraniEtAl2005
%  LocalWords:  HarchaouiLevy Leduc2010 ChanYauZhang2014
%  LocalWords:  BhadraDattaPolsonEtAl2015a CarvalhoPolsonScott2009
%  LocalWords:  DattaGhosh2012 PasKleijnVaart2014a
