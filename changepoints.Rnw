
\section{Introduction}
\label{sec:introduction}

This paper 


\begin{itemize}
\item How change-points are typically 
\item Representation as a variable selection problem
\item Discussion of shrinkage priors
\item Dynamic linear model representation
\end{itemize}

\section{}

Consider the case of observations with a mean that takes on $M$ values with change-point locations, $\tau_{1}, \dots, \tau_{m}$,
\begin{equation}
  \label{eq:1}
  \begin{aligned}[t]
    y_{t} & \mu_{t} + \epsilon_{t} \\
    \mu_{t} &= \mu^{*}_{m} & \text{if $\tau_{m} \leq t \leq \tau_{m + 1}$}
  \end{aligned}
\end{equation}
for $t = 1, \dots, n$, and with the convention that  $\mu_{t} = \mu^{*}_{1}$ and $\mu_{t} = \mu^{*}_{M}$.
In the equation, $\epsilon_{t}$ are mean zero and iid random variables.
This case is presented for convenience, and I will consider more general cases later.

Instead of focusing on the locations of the change-points, the problem in equation (\ref{eq:1}) can be rewritten to focus on the values of the changes $\mu_{t} - \mu_{t - 1}$.
\begin{equation}
  \label{eq:2}
  \begin{aligned}
    y_{t} &= \mu_{t} + \epsilon_{t} \\
    \mu_{t} &= \mu_{t - 1} + \omega_{t}
  \end{aligned}
\end{equation}
In change-points, the innovations $\omega_{t}$ are sparse, \ie{}most values of $\omega_{t}$ are zero, and only a few are non-zero.
The change-points are the times at which $\omega_{t}$ are non-zero.
In this formulation, the problem is one of variable selection, finding the non-zero values of $\omega_{t}$ and estimating their values.
\footnote{In Equation (\ref{eq:2})The initial value of $\mu_{1}$, or equivalently $\mu_{0}$, needs to be given a value.
%This will likley work better with the formulation that initilizes at t = 1, since we should assume \omega_1=0.
}

The natural approach to variable selection is a two-groups model (Efron 2008, ...)
This means that the variable has a prior distribution that is a mixture of a point mass at zero, and a diffuse distribution.
\begin{equation}
  \label{eq:3}
  \omega_{t} = \pi f(\omega) + (1 - \pi) \delta_{0}
\end{equation}
\textcite{GiordaniKohn2008} propose this solution to change-points and provide an efficient implement

Recent advances in Bayesian computation has focused on one-group solutions to the variable selection problem.
This are single continuous distributions (or mixtures of continuous distributions) that approximate the mixture distribution in Equation (\ref{eq:3}) by have a spike at zero, and fat tails.
These are similar in spirit to the regularization literature of the LASSO and variations thereof, that handle shrinkage and variable selection simultaneously.

In the Bayesian literature there are several proposed distributions.
See ... for an overview.

\begin{itemize}
\item Double Exponential
\item Horseshoe
\item Horseshoe+ 
\end{itemize}


Shrinkage approaches estimate sparsity using a continuous prior distribution centered at zero.
Early examples of shrinkage approaches include \parencites{Tibshirani1996}{Tipping2001}.
Shrinkage priors are the Bayesian equivalent of penalized likelihood and regularized regression.
Most proposed Bayesian shrinkage priors are in the the class of global-local mixtures of normal distributions \parencite{PolsonScott2010}:
\begin{equation}
  \label{eq:3}
  \begin{aligned}[t]
    \theta_{t} &= N(0, \tau^{2} \lambda_{i}^{2}) \\ 
    \lambda_{t}^{2} &\sim p(\lambda_{t}^{2}) \\
    \tau_{t}^{2} &\sim p(\tau^{2})
  \end{aligned}
\end{equation}
A global-local normal distribution is a scale mixture of normal distributions in which the variance is the product of a global variance component $\tau^{2}$ and a local variance component $\lambda_{t}^{2}$.
The $\lambda_t$ are also called mixing parameters, and their distribution is called the mixing distribution.
Distributions in the global-local class of priors differ in the assumed distribution of the local variance component. 
Examples of commonly used shrinkage distributions and their mixing distributions include \parencite{PolsonScott2010}:%
\footnote{A normal prior, as used in ridge regression, is the trivial case in which $\lambda_{i} = 1$ for all $i$.}
\begin{description}
\item[Student's t] The mixing distribution is inverse gamma, $p(\lambda_{i}) = \dinvgamma{\xi / 2, \xi \tau^{2} / 2}$ where $\xi$ are the degrees of freedom. \parencite{Tipping2001}
\item[Double Exponential (Lasso)] The mixing distribution is exponential \parencites{ParkCasella2008}{Hans2009},
\begin{equation*}
    \begin{aligned}[t]
      p(\lambda_{i} | \tau_{i}) &= \frac{1}{2 \tau^{2}} \exp (\lambda_{i}^{2} / 2 \tau^{2}) \\
      p(\tau^{2}) &\sim \dinvgamma{\xi / 2, \xi d^{2} / 3}
    \end{aligned}
  \end{equation*}
\item[Normal-Gamma] The mixing distribution is gamma, $\lambda^{2}_{i} \sim \dgamma{a, b}$ \parencites{CaronDoucet2008}{BrownGriffin2010}.
\item[Horseshoe] The mixing distribution is inverse-beta $\lambda^{2}_{i} \sim \dinvbeta{\frac{1}{2}, \frac{1}{2}}.$ 
Equivalently, $\lambda_{i} \sim \dhalfcauchy{0, 1}$ \parencite{CarvalhoPolsonScott2010}.
\end{description}

\section{Estimation}
\label{sec:estimation}

\subsection{Dynamic Linear Models}
\label{sec:dynam-line-models}


\subsection{Estimation in Stan}
\label{sec:estimation-stan}

\section{Change Point Examples}
\label{sec:examples}









\section{Efficient Estimation of DLMs in Stan}
\label{sec:effic-estim-dlms}

Comparison of Kalman filter FFBS implementations: @DurbinKoopmans2012
Other comparisons of sampling schemes: @ReisSalazarGamerman2006.
Comparison of speed of implementations in R, focusing on mode finding: @Tusell2011.


Marginalizing over latent states is necessary in discrete state space models in \Stan{} since HMC does cannot sample discrete parameters CITE.


%  LocalWords:  iid Efron GiordaniKohn2008 Tibshirani1996 Tipping2001
%  LocalWords:  PolsonScott2010 ParkCasella2008 Hans2009 DLMs Kalman
%  LocalWords:  CaronDoucet2008 BrownGriffin2010 FFBS Tusell2011 HMC
%  LocalWords:  CarvalhoPolsonScott2010 DurbinKoopmans2012
%  LocalWords:  ReisSalazarGamerman2006
