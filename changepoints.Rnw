<<echo = FALSE, results = 'hide', message = FALSE, warning = FALSE, error = FALSE>>=
library("bsdlm")
data("BushApproval", package = "bsdlm")
tab_path <- file.path(PROJECT_ROOT, "analysis", "tex")
fig_path <- file.path(PROJECT_ROOT, "analysis", "figures")
date_format <- function(x) format(x, "%B %d, %Y")


@ 
\newcommand{\ModelII}[1]{\texttt{#1}}


\section{Introduction}
\label{sec:introduction}

Political and social processes are rarely, if ever, constant over time.

Political science paper on change-points \parencites{CalderiaZorn1998}{WesternKleykamp2004}{Spirling2007a}{Spirling2007b}{Park2010}{Park2011}{Blackwell2012}.

Thus, political scientists often have a need to estimate time-varying parameter (TVP) models.
There exist two broad approaches to estimating time-varying parameters: structural break approaches, which include dummy variables and change point models, and smoothing approaches, which include dynamic linear models and smoothing splines. 
Both approaches estimate TVP when the number of observations is small relative to the number of time periods. 
Smoothing approaches keep the parameter differences small in all periods, while structural break approaches restrict parameter changes to a few locations. 
These two approaches are often viewed as distinct and estimated using different methods, forcing the researcher to choose between which model to use.
Many processes the changes in the process are characterized by many periods of stability and a few periods of possibly rapid and large, change (Ratkovic and Eng 2010).
Historically, existing smoothing methods have had a difficulty estimating these sorts of processes, so structural break models have been used

This paper presents a simple and extensible method to estimate time-varying parameters that may be subject to structural breaks. 
Time-varying parameters with possible structural breaks can be estimated within a continuous state-space model, and in particular as a dynamic linear model, by placing a shrinkage prior on the distribution of the state disturbances.
The intuition behind this can be illustrated with a simple model.
Suppose there is a vector of observed data, $y_{1}, \dots, y_{n}$, drawn from a normal distribution with a time varying mean, $\mu_{1}, \dots, \mu_{n}$.
This can be represented as a dynamic linear model,

The difference between "structural breaks" and "smoothing" data-generating processes and estimation techniques is whether the XXXX vector is sparse (most XXXX = 0) or dense (most XXXX neq 0).
A common model is to apply a normal distribution to XXXX
A normal XXXX  estimate change over time, but it cannot produce sparse estimates of XXXX.
The thin tails of the normal distribution will tend to over-smooth periods around structural breaks, under-smooth periods in which there were no structural breaks.
However, estimating sparse parameter vectors is a general problem that has received considerable attention lately in large-p, small-n problems (Tibshirani1996 1996; Polson and Scott 2010). This paper applies some of the advances in sparse parameter estimation to estimating time-varying parameters with structural breaks. Structural breaks, i.e. sparse, can be estimated by placing a shrinkage prior on.
While there are a large number of Bayesian shrinkage priors proposed, this paper will use the Horseshoe prior (HS) distribution introduced in Carvalho, Polson, and Scott (2009) and Carvalho, Polson, and Scott (2010). I will refer to a dynamic linear models with shrinkage priors on the state disturbances as a sparse disturbance dynamic linear models (SDDLM).

This method does not require specifying the number of structural breaks ex ante.
\begin{enumerate}
\item The sparsity of XXXX will determine the number of structural breaks, and this sparsity can be estimated from the data.
Structural breaks can be detected from the posterior distribution of XXX using several rules. Not only do
the number of structural breaks need not be specifieded beforehand, this method will work reasonable
well even if the underlying data-generating process is dense, \ie{}the parameter changes in each period.
\item This method is extensible. 
  Dynamic linear models incorporate a wide variety of models, including ARIMA
  and structural time-series, cubic splines, and regressions with time-varying coefficients. Any model in
  which the parameter of interest can be expressed as a latent state in a dynamic linear model can be
  altered to assign a shrinkage prior to the state disturbance in order to make it robust to or to detect
  structural breaks for that parameter. While many structural break methods are specific to single
  models, this paper shows how sparse-disturbance DLMs can be estimated within the general purpose
  Bayesian software, \Stan{}.
\item The method allows for easy estimation of structural breaks in multiple parameters which can be either
independent or correlated. Estimating independent structural breaks in a discrete state space model,
like change-point models, results in the state-space multiplying exponentially.
assigned a shrinkage prior.
\item This method is efficient in both programmer and computational time, while still retaining the extensibility
to estimate a wide variety of models. Since most shrinkage priors, including the HS distribution
used in the paper, are scale-mixture of normal distributions, this state space model estimated is a
(conditionally) Gaussian dynamic linear model (GDLM). There are can take advantage of the computationally
efficient methods of mode finding and sampling from GDLMs, such as the Kalman filter and
Forward-Filter Backwards-Sample. This paper shows how a combination of Stan, a general purpose
Bayesian software program, and R can be used to easily estimate and sample from the posterior of
dynamic linear models.
\end{enumerate}

First, if change points could be understood in a weaker sense. 
In few political processes is there actually no change in periods.
Instead, change is non-zero, but slow in most periods, with some periods showing rapid change.

Second, even though change point models directly estimate the location of the change points, after averaging over the posterior probability of the change points, the posterior distribution of the changes in the parameter will not be sparse. 

Third, using these sorts of one-group models may have some benefits in either flexibility in terms of the types of models which can be estimated, or in the computational speed.

Note that these models, like many sparsity inducing models, often conflate two distinct objectives. 
The first goal of these models is to improve in- or out-of-sample fit which the data generating process actually has a few very large signals and many zero or relatively small signals.
The second goal of these models is interpretation for the researcher.
It is easier to present results that ``there is a change point at time $t$'' than ``the data is slowly trending, and it seems to be trending sharply around $t$, but then it does not move as much after that.''
In this case, sparsity is not (directly) an attempt to model the data generating process, but a means to get a more interpretable model.
In many contexts performing statistical inference on ``how many change points?'' or ``what is the probability that $t$ is a change point?'' is non-sensical, since it is almost certain that there a no ``true'' change points, and finding the right number of change points is the researcher choosing a tradeoff between how well the model represents a complex reality, and how interpretable the model is.
The method presented here does not produce perfectly sparse models, nor does provide probabilities that certain points are or are note change points. 
Therefore, it is better suited to the first case, of modeling cases in which the 


\section{Change points as a Variable Selection Problem}
\label{sec:chang-as-vari}

For simplicity, I start with the case of change-points in the mean, and later generalize to other cases.
In this case, there are $n$ ordered observations, $y_{1}, \dots, y_{n}$, with a time-varying mean, $\mu_{t}$.
\begin{equation}
  \label{eq:17}
  \begin{aligned}[t]
    y_{t} & = \mu_{t} + \epsilon_{t} && \text{$\epsilon_{t}$ are iid with $\E(\epsilon) = 0$.} \\  
  \end{aligned}
\end{equation}
Suppose that there are $M$ change points, with ordered change-point locations, $\tau_{1}, \dots, \tau_{m}$, with the convention that $\tau_{0} = 0$ and $\tau_{M + 1} = n$.
This splits the mean into $M + 1$ segments, with values of the mean $\mu^{*}_{0}, \dots, \mu^{*}_{M}$,  such that
\begin{equation}
  \label{eq:1}
  \begin{aligned}[t]
    \mu_{t} &= \mu^{*}_{m} && \text{if $\tau_{m} \leq t \leq \tau_{m + 1}$}
  \end{aligned}
\end{equation}
There are a variety of change-point approaches to this problem in both classical \parencites{Page1954a}{Hinkley1970a}{BaiPerron2003a}{OlshenVenkatramanLucitoEtAl2004}{BaiPerron1998}{KillickFearnheadEckley2012} and Bayesian statistics \parencites{Yao1984}{BarryHartigan1993}{Chib1998}{Fearnhead2006a}{FearnheadLiu2007a}.

An alternative approach to the change point problem is to rewrite the problem in Equation (\ref{eq:1}) to focus on the changes in the mean (innovations), $\omega_{t} = \Delta \mu_{t} = \mu_{t} - \mu_{t - 1}$, rather than the locations of the change points:
\begin{equation}
  \label{eq:2}
  \begin{aligned}[t]
    \mu_{t} &= \mu_{t - 1} + \omega_{t}
  \end{aligned}
\end{equation}
In a change points situation, the innovations $\omega_{t}$ are sparse, meaning that most values of $\omega_{t}$ are zero, and only a few are non-zero.
The times of the change points are not directly estimated, but are those times at which $\omega_{t}$ are non-zero.
This turns the problem from one of segmentation, to one of variable selection, in which the goal is to find the non-zero values of $\omega_{t}$ and estimate their values.
\footnote{In Equation (\ref{eq:2}) the initial value of $\mu_{1}$, or equivalently $\mu_{0}$, needs to be given a value.
%This will likley work better with the formulation that initilizes at t = 1, since we should assume \omega_1=0.
}
The natural approach to this variable selection problem is to explicitly model the values of $\omega$ as a discrete mixture between a point mass at zero for the non-change points, and a heavy-tailed alternative \textcite{MitchellBeauchamp1988a}{Efron2008a}:
\begin{equation}
  \label{eq:3}
  \omega_{t} = \rho g(\omega) + (1 - \rho) \delta_{0}
\end{equation}
where $g$ is some heavy-tailed distribution, and $\delta_{0}$ is a Dirac delta distribution (point mass) at 0.
Equation (\ref{eq:3}) is often called a ``spike and slab'' prior \textcite{MitchellBeauchamp1988a}.
Since it explicitly models the two groups (zeros and non-zeros), \textcite{Efron2008a} calls this the two-group answer to the two-groups problem.
This prior is convenient because it directly provides for each time a posterior probability that it is a change point (non-zero).
\textcite{GiordaniKohn2008} propose using the representation in Equation (\ref{eq:2}) with the a discrete mixture distribution for $\omega$, as in Equation (\ref{eq:3}), to estimate change points.

Recent work in Bayesian computation has focused on one-group solutions to the variable selection problem, which combine shrinkage (regularization) and variable selection through the use of continuous distribution with a large spike at zero and wide tails \parencite{}
This is analogous to the sparse regularization literature in frequentist statistics spawned by LASSO \parencite{Tibshirani1996} and its the numerous variations.
In this approach, several papers have proposed using LASSO-type penalties in a maximum likelihood approach to estimate change-points \parencites{TibshiraniEtAl2005}{HarchaouiLevy-Leduc2010}{ChanYauZhang2014}.
In the Bayesian literature or shrinkage in sparse situations numerous distributions have been proposed. 
This paper will look at four of them: the Student's $t$ distribution \parencite{Tipping2001}, the Double Exponential or Laplace \parencite{ParkCasella2008}{Hans2009}, the Horseshoe \parencite{CarvalhoPolsonScott2010}, and Horseshoe+ \parencite{BhadraDattaPolsonEtAl2015a}.


\begin{description}

\item[Student's $t$] The Student's $t$ distribution for $x \in \R$  with scale $\tau \in \Rp$ and degrees of freedom $\nu \in \Rp$,
\begin{equation}
  \label{eq:6}
  p(\omega_{t} | \tau, \nu) = \frac{\Gamma\left(\frac{\nu + 1}{2}\right)}{\sqrt{\nu \pi} \Gamma\left( \frac{\nu}{2} \right)} \left( 1 + \frac{\omega_{t}^{2}}{\nu} \right)
\end{equation}
The Student's $t$ distribution can also be expressed as a scale mixture of normals,%
\footnote{
  $\dinvgamma{x | \alpha, \beta}$ is an inverse-gamma distribution for $x \in \Rp$ with shape $\alpha \in \Rp$ and inverse-scale $\beta \in \Rp$,
  \begin{equation}
    \label{eq:9}
    \dinvgamma{x | \alpha, \beta} = \frac{\beta^{\alpha}}{\Gamma(\alpha)} x^{-(\alpha - 1)} \exp 
    \left(
      \beta \frac{1}{x}
    \right)
  \end{equation}
}
\begin{equation}
  \label{eq:5}
  \begin{aligned}[t]
  \omega_{t} | \tau, \nu, \lambda_{t} &\sim \dnorm{0, \tau^{2} \lambda_{t}^{2}} \\
  \lambda_{t}^{2} | \nu & \sim \dinvgamma{\frac{\nu}{2}, \frac{\nu}{2}}
  \end{aligned}
\end{equation}
\parencite{Tipping2001} uses the Student's $t$ for sparse shrinkage by letting the degrees of freedom $\nu \to 0$.

\item[Double Exponential] The double exponential (Laplace) with scale $\tau$,
\begin{equation}
  \label{eq:14}
  p(\omega_{t}| \tau) = \frac{1}{2 \tau} \exp 
  \left(
    - \frac{\abs{\omega_{t}}}{\tau}
  \right)
\end{equation}
The double exponential distribution can also be expressed as a scale-mixture of normal distributions,%
\footnote{
  $\dexp{x | \beta}$ is the exponential distribution for $x \in \Rp$ with inverse-scale (rate) parameter $\beta \in \Rp$,
    \begin{equation}
      \label{eq:13}
      \dexp{x | \beta} = \beta \exp(- \beta x)
    \end{equation}
}
\begin{equation}
  \label{eq:12}
  \begin{aligned}[t]
  \omega_{t} | \tau, \lambda_{t} &\sim \dnorm{0, \tau^{2} \lambda_{t}^{2}} \\
  \lambda_{t}^{2} & \sim \dexp{\frac{1}{2}}
  \end{aligned}
\end{equation}
The double exponential distribution is the distribution that corresponds to the $\ell_{1}$ penalty used in LASSO \parencites{ParkCasella2008}{Hans2009}.
The LASSO achieves sparsity due to the spike in the double exponential distribution at zero places also places the mode of estimates at zero. 
However, the although the double exponential distribution has a spike at zero, it does not produce sparse posterior means since it has relatively little mass close to zero.
Additionally, the tails of the double exponential are narrow, so it tends to excessively shrink large signals \parencites{CarvalhoPolsonScott2010}.

\item[Horseshoe] The horseshoe distribution \parencite{CarvalhoPolsonScott2009}{CarvalhoPolsonScott2010} is defined hierarchically as a scale-mixture of normals
\begin{equation}
  \label{eq:7}
  \begin{aligned}[t]
    \omega_{t} | \lambda_{t}, \tau & \sim \dnorm{0, \tau^{2} \lambda_{t}^{2}} \\
    \lambda_{t}  & \sim \dhalfcauchy{0, 1}
  \end{aligned}
\end{equation}
where $\dhalfcauchy{x | 0, s}$ denotes half-Cauchy distribution with a scale parameter $s$, and density
\begin{equation}
  \label{eq:8}
  p(x | s) = \frac{2}{\pi x \left(1 + {\left(\frac{x}{s}\right)}^{2}\right)}
\end{equation}
The Horseshoe distribution has some theoretically attractive properties for shrinkage and variable selection \parencites{CarvalhoPolsonScott2009}{CarvalhoPolsonScott2010}{DattaGhosh2012}{PasKleijnVaart2014a}.

\item[Horseshoe+] The horseshoe+ distribution \textcite{BhadraDattaPolsonEtAl2015a} is similar to the Horseshoe distribution, but with an additional term,
\begin{equation}
  \label{eq:15}
  \begin{aligned}[t]
    \omega_{t} | \lambda_{t}, \eta_{t}, \tau & \sim \dnorm{0, \tau^{2} \lambda_{t}^{2}} \\
    \lambda_{t}  & \sim \dhalfcauchy{0, \eta_{t}} \\
    \eta_{t} & \sim \dhalfcauchy{0, 1} \\
  \end{aligned}
\end{equation}
\end{description}

The shrinkage distributions considered here also global-local scale mixtures of normals, with a global variance component $\tau$, and local variance components $\lambda_{t}$ \parencite{PolsonScott2010}.
The global variance component $\tau$ concentrates the distribution around zero, while the local variance components allow individual values to be far from zero.

To recap, the proposed model for change points is
\begin{align}
  \label{eq:16}
  y_{t} & \sim \epsilon_{t} & \text{$\epsilon_{t}$ iid, $\E(\epsilon) = 0$, $\Var(\epsilon) = \sigma^{2}$} \\
  \mu_{t} &= \mu_{t - 1} + \sigma \omega_{t}
\end{align}
where $\omega_{t}$ is given a shrinkage prior distribution that induces sparsity.
This work will propose horseshoe and horseshoe+ distributions as those shrinkage priors, and compare them other shrinkage priors such as the Student's $t$ and double exponential distributions.


\begin{figure}[!htpb]
 \begin{subfigure}[b]{\linewidth}
  \includegraphics{\Sexpr{file.path(PROJECT_ROOT, "analysis", "figures", "dist-plot_zeros-1")}}
 \end{subfigure}
 \begin{subfigure}[b]{\linewidth}
    \includegraphics{\Sexpr{file.path(PROJECT_ROOT, "analysis", "figures", "dist-plot_tails-1")}}
 \end{subfigure}
  \caption{
    Comparison of the density functions of normal, Cauchy, Laplace, horseshoe, and horseshoe+ distributions.
    All functions have location and scale parameters of 0.
    The Cauchy distribution is a special case of the Student's $t$ distribution with degrees of freedom equal to 1.
  }
  \label{fig:density}
\end{figure}




\section{Discussion}
\label{sec:ident-change-points}

The method proposed here is better at estimating the values of $\mu_{t}$, or rather the posterior distribution, when change-points may be present than providing probabilities of the locations of the change-points themselves.
The first method uses the posterior distribution to detect structural breaks is simply a test using posterior distribution of $\omega$.
This is the Bayesian equivalent to the auxiliary residual test of \textcite{DeJongPenzer1998}.
In addition to the use of a credible interval rather than a confidence interval, the Bayesian method differs from the auxiliary residual test in that it integrates over the posterior distribution of the state matrices rather than fixing them at point estimates.


% The second method uses the local variance components $\lambda_{t}$ to identify structural breaks \parencite[179-180]{PetrisPetroneEtAl2009}.
% If $\lambda_{t}$ = 1, then the state disturbance is distributed normal with a scale equal to the global scale.
% Thus, the local shrinkage parameters $\lambda_{t}$ are relative measure of how much each $\omega_{t}$ is shrunk towards zero.
% Values of $\lambda_{t} > 1$ ($\lambda_{t} < 1$ indicate state disturbances that are dispersed (shrunk) relative to the global scale.
% A structural break can be classified as a time period in which $\E(\lambda_{t}) > 1$.

% The third method uses the contribution of observations to the state. 
% Talk about Kalman gain and smoother

Finally, for many practical purposes and interpretation, an ``eyeball test'' by the researcher by plotting the mean or median of the posterior estimates of $\mu$ should suffice.As noted earlier, for many data generating processes, the hypothesis that $\omega_{t} = 0$ is implausible, so testing for change points does not make much sense.
Rather, change point models are used to ease interpretation, so an informal method of visualizing is enough.
If $\Delta \mu$ is not distinguishable in a plot, then it is unlikely to be of any practical import.

While this one-group approach does not provide as clear an estimate of the locations or existence of change points, it is useful for several reasons.
\begin{enumerate}
\item For many time-varying parameter processes typically modeled with a change-point model, the situation akin to a situation in which the parameter is changing in all periods, but most of these changes are small relative to that of a few periods.
  In other words, the innovations $\omega$ are never zero; it is just that in most periods they are relatively small.
  This seems plausible for many processes modeled by political scientists in which there is no physical reason to think there are discrete states, apart from perhaps institutional changes.
  It may be useful for the researcher to model it with a step function (change points) for parsimony and interpretation, but the data-generating process is likely closer to one in which things are usually changing a little, but in some periods they change a lot. 
  I would argue that the data generating processes considered by political science papers using change points falls into this category, for example Supreme court dissent and consensus \parencite{CalderiaZorn1998}, wage growth in OECD states \parencite{WesternKleykamp2004}, casualties in the Iraq War \parencite{Spirling2007a}, presidential use of force \parencite{Park2010}, and campaign contributions \parencite{Blackwell2012}.
\item When the goal is estimation or prediction of the parameters, rather than the locations of the change-points, then the changes in the parameter is continuous anyways after integrating over the posterior distribution of the change-points. 
  The one-group approach approximates this distribution.
\end{enumerate}


\section{Estimation and Implementation in \Stan{}}
\label{sec:estimation}

A helpful feature of the model as laid out in the previous section is that it that it can be expressed as Gaussian dynamic linear model, which has properties useful for its computation and generalization.
A Gaussian dynamic linear model (GDLM), or linear Gaussian state space model, is a model represented by the system of equations \parencites{DurbinKoopman2012}{WestHarrison1997}{PetrisPetroneEtAl2009}[Ch 6]{ShumwayStoffer2010},
\begin{align}
  \label{eq:4}
  y_{t} &\sim \dnorm{b_{t} + F_{t} \theta_{t}, V_{t}} \\
  \label{eq:24}
  \theta_{t} &\sim \dnorm{g_{t} + G_{t} \theta_{t - 1}, W_{t}} 
\end{align}
In these equations, the observed date , $y_{t}$, is a linear function of some latent states, $\theta_{t}$, which are a function of their previous values $\theta_{t-1}$.
Equation (\ref{eq:4}) is the \textit{observation equation}, where $y_{t}$ (observation vector) is a $r \times 1$ vector of observed data, $\theta_{t}$ (state equation) is a $p \times 1$ vector of unobserved states, $b_{t}$ is an $r \times 1$ vector, $F_{t}$ is a $r \times p$ matrix, and $V_{t}$ (observation variance) is an $r \times r$ covariance matrix.
Equation (\ref{eq:24}) is the \textit{state equation} equation, which relates the current state to its previous value; $g_{t}$ is an $p \times 1$ vector, $G_{t}$ is a $p \times p$ matrix, and $W_{t}$ (state variance) is an $p \times p$ covariance matrix.
The vectors and matrices, $\Phi = \{ b_{t}, g_{t}, F_{t}, G_{t}, V_{t}, W_{t} \}_{t \in 1:n}$ are \textit{system matrices}, which in applications will often be functions of parameters.
GDLMs are a general class of models which includes many common time series models, including SARIMA models, structural time series \parencite{Harvey1990}, dynamic factor models, seemingly unrelated regression, and linear regression with time varying coefficients, among others \parencite[Ch. 3]{DurbinKoopman2012}. 

The model in Section \ref{sec:chang-as-vari}, Equations \eqref{eq:17} and \eqref{eq:2}, is a GDLM.
In this case, $\beta_{t} = g_{t} = 0$, $F_{t} = G_{t} = 1$, and $\epsilon_{t}$ in Equation (\ref{eq:17}) are distributed $\epsilon_{t} \sim \dnorm{0, V_{t}}$, and $\omega_{t} \sim \dnorm{0, V_{t}}$.
Even though the proposed distributions distributions for $\omega_{t}$ are not normal, since they are all representable as scale-mixtures of normal distributions, they are normal conditional on the values of $\lambda_{t}$.%
These models are similar to what is often called the \textit{local level model} \parencite[Ch 2.]{DurbinKoopman2012}, except that they have time-varying state variances, $W_{t}$.
In other words, the change point model discussed here is simply a local level model with a sparsity-inducing shrinkage prior on the state variance.

That these change point models can be represented as linear Gaussian state space models is useful for two reasons.
First, it suggests how these models can be extended beyond simple changes in means.
Any sort of GDLM, which includes many common models, can be adjusted to account for change points in in any of its states, by using a shrinkage prior distribution for its system variance, $W$.%
\footnote{Though the shrinkage prior distribution will need to be a scale-mixture of normals.}

Second, since both the observation and system equations are multivariate normal, there are analytical solutions that allow for efficiently computing its likelihood and sampling the latent states from their posterior distribution.
The Kalman filter calculates the values of $p(\theta_{t} | y_{1:(t-1)})$  and $(\theta_{t}| y_{1:t})$, and can be used to calculate the likelihood of $p(y | \Phi)$.
Importantly, the Kalman filter calculate the likelihood of $y$ without needing the values of the latent states $\theta$.
The derivations of the Kalman filter can be found in most time-series texts, including \textcite[Ch. 5--7]{DurbinKoopman2012}{WestHarrison1997}.
Given the results of the Kalman filter there are several methods to sample $\theta$ from $p(\theta | y, \Phi)$, a process called Forward-Filtering Backwards-Smoothing (FFBS) or simulation smoothing \parencites{CarterKohn1994}{Fruehwirth-Schnatter1994}{DeJongShephard1995}{DurbinKoopman2002}[Ch 4.9]{DurbinKoopman2012}.
I make use of these methods to efficiently sample both the latent states, $\theta$, and other parameters of the model in \Stan{}.

\Stan{} is a probabilistic programming language, with its own BUGS-like language, and interfaces to several programming languages, including \RLang{} \parencites{Stan2015a}{CarpenterGelmanHoffmanEtAl2015a}.
GDLMs could be estimated in \Stan{} by directly translating the models described Equations (\ref{eq:4}) and (\ref{eq:24}) into a \Stan{} model.%
\footnote{Since these models are hierarchical, it will be more efficient to implement them using the computational tricks in Section ... of X.}
Although the horseshoe and horseshoe+ distributions are not directly implemented in \Stan{} since they have not closed form, but they are easily implemented in Stan through their hierarchical representation.

However, GDLMs can be estimated more efficiently in \Stan{} by marginalizing over the latent states.
The sampling methods implemented in \Stan{}, the primary of which is HMC-NUTS \parencites{HoffmanGelman2014a}, only require the calculation of a likelihood from the user.%
\footnote{Technically, it also requires derivatives of the likelihood, but these are generated automatically by \Stan{} using its automatic differentiation engine.}
The Kalman filter can be used to calculate the likelihood $p(y | ...)$ by marginalizing over the latent states, $\theta_{t}$. 
Marginalizing over parameters is required when estimating models with discrete parameters in \Stan{}, e.g. mixture models \parencite[104]{Stan2015a}.
Although it is not necessary to sample from GDLMs, it helps the efficiency of sampling by reducing the correlation between the latent states and the other parameters of the model.%
If needed, the latent states can be sampled using a simulation smoothing method discussed earlier.%
\footnote{In \Stan{}, the calculation of $p(y| .)$ is done in the \texttt{transformed parameter} or \texttt{model} blocks, while the sampling of the latent states is done in the \texttt{generated quantities} block.}
This two-step process is an example of partially collapsed Gibbs-sampling \parencite{VanDykPark2008a}.

To summarize, an efficient method to sample GDLMs in \Stan{} is:
\begin{enumerate}
\item Sample $\vartheta$ from $p(\vartheta | y)$ using HMC in \Stan{}.
  This requires integrating out the latent states, $\theta$, and calculating $p(y | \vartheta)$, which is done using a Kalman filter.
\item Sample $p(\theta | y, \vartheta)$ using a simulation smoother for a GDLM as in  \parencites{CarterKohn1994}{Fruehwirth-Schnatter1994}{DeJongShephard1995}{DurbinKoopman2002}[Ch 4.9]{DurbinKoopman2012}.
\end{enumerate}
I use this method to sample all the models in this work.
This required implementing the Kalman filter and simulation smoothing methods in \Stan{}.
I provide a full set of user-defined \Stan{} functions that implements Kalman filters, smoothers, and simulation sampling available at \textcite{Arnold2015c}.
Section \ref{sec:example-stan-program} provides the code for one of the \Stan{} modules used in this paper.%
\footnote{The full code for all models run in this work are available at \url{https://github.com/jrnold/dlm-shrinkage}.}

% This implementation of state space models in \Stan{} is one of the few state space specific samplers in general Bayesian programming language.
% Several \RLang{} packages implement Kalman filtering, smoothing, and simulation for Gaussian state space models; see \textcite{Tusell2011} for an overview.
% However, these do not provide a full Bayesian sampling solution, since the user would still need to write their own sampler for any other parameters going into the state space model, or if the state space model were a component of a more complex model.
% The  probabilistic programming language, \proglang{JAGS}, does not implement a specific sampler for Gaussian linear state space models.


\section{Example: Nile}
\label{sec:nile}

A classic dataset that has been analyzed in many works and texts on times series and structural breaks is the annual flow volume of the Nile River between 1871 and 1970. \parencites{Cobb1978}{Balke1993}{DeJongPenzer1998}{}{DurbinKoopman2012}{CommandeurKoopmanOoms2011}.%
\footnote{The dataset is included with \RLang{} as \texttt{Nile} in the included package \pkg{datasets}.}
The series seems to show a single large shift in the average annual flow around 1899.
This shift in the level is attributed to construction of a dam at Aswan that started operation in 1902 or climate changes that reduced rainfall in the area \parencite[278]{Cobb1978}.

\begin{figure}
  \centering
  \includegraphics{\Sexpr{file.path(PROJECT_ROOT, "analysis", "figures", "nile-nile-1")}}
  \caption{Annual flow of the Nile River, \Sexpr{start(datasets::Nile)[1]}--\Sexpr{end(datasets::Nile)[1]}. Previous work has found a break point in this series near 1899.}
  \label{fig:nile}
\end{figure}


I will compare several models of the annual flow.
In all models, $y_{t}$ be the annual flow of the Nile River,%
\footnote{Discharge at Aswan in $10^8 m^{3}$.} %
from $t \in \Sexpr{start(datasets::Nile)[1]}, ..., \Sexpr{end(datasets::Nile)}$ with $n = \Sexpr{length(datasets::Nile)}$ observations, 
where the observations are normally distributed given the mean $\mu_{t}$,%
\footnote{
  This may not be appropriate since that data appear to contain possible outliers at 1879, 1913, and 1964.
  However, modeling outliers is not the purpose of this analysis.
}
\begin{equation}
  \label{eq:10}
  \begin{aligned}[t]
    y_{t} &\sim \dnorm{\mu_{t}, \sigma^{2}}
  \end{aligned}
\end{equation}
The various models differ in how they model the possibly time-varying mean, $\mu_{t}$.
The models I consider are:
\begin{description}[font = \normalfont\ModelII]
\item[Constant] In this model, the mean is constant.
  \begin{equation}
    \label{eq:21}
    \begin{aligned}[t]
      \mu_{t} &= \mu_{0} && \text{for all $t$}
    \end{aligned}
  \end{equation}
\item[Intervention] This includes an indicator variable for all years including and after 1899.
  This models the situation in which the researcher knows, correctly, the change points.
  \begin{equation}
    \mu_{t} = 
    \begin{cases}
      \mu_{0} & t < 1899 \\
      \mu_{0} + \omega & t \geq 1899
    \end{cases}
  \end{equation}
\item[Normal] In this model, the innovations are distributed normal.
  This corresponds to the typical local level mode \parencite[Ch. 2]{DurbinKoopman2012}[Ch. 2]{WestHarrison1997}.%
  \footnote{For example, as implemented in the \RLang{} function \texttt{StructTS}.}
  \begin{equation}
    \label{eq:11}
    \begin{aligned}[t]
      \mu_{t} &= \mu_{t - 1} + \omega_{t} & \omega_{t} & \sim N(0, \tau^{2})
    \end{aligned}
  \end{equation}
\item[StudentT] In this model, the innovations are distributed Student $t$.
  The prior distribution of the degrees of freedom parameter $\nu$ is that suggested by \textcites{JuarezSteel2010b}, a Gamma distribution with mode 10, mean 20, and variance 200, which places most of mass in the relevant region of the space.
  \begin{equation}
    \label{eq:18}
    \begin{aligned}[t]
      \mu_{t} &= \mu_{t - 1} + \omega_{t} & \omega_{t} & \sim \dt{\nu}{0, \tau}
    \end{aligned}
  \end{equation}
\item[Laplace] In this model, the innovations are distributed Laplace (Double Exponential):
  \begin{equation}
    \label{eq:22}
    \begin{aligned}[t]
      \mu_{t} &= \mu_{t - 1} + \omega_{t} & \omega_{t} & \sim \dlaplace{0, \tau}
    \end{aligned}
  \end{equation}
\item[Horseshoe] In this model, the innovations are distributed horseshoe.
  The half-Cauchy parameter for the global scale parameter $\tau$ is suggested for the horseshoe and horseshoe+ parameters in \textcite{BhadraDattaPolsonEtAl2015a}.
  \begin{equation}
    \label{eq:19}
    \begin{aligned}[t]
      \mu_{t} &= \mu_{t - 1} + \omega_{t} & \omega_{t} & \sim \dnorm{0, \tau^{2} \lambda^{2}_{t}} \\
      \lambda_{t} &\sim \dhalfcauchy{0, 1} \\
      \tau &\sim \dhalfcauchy{0, \frac{1}{n}}
    \end{aligned}
  \end{equation}
\item[Horseshoe+] In this model, the innovations are distributed horseshoe.
  The half-Cauchy parameter for the global scale parameter $\tau$ is suggested for the horseshoe and horseshoe+ parameters in \textcite{BhadraDattaPolsonEtAl2015a}.
  \begin{equation}
    \label{eq:23}
    \begin{aligned}[t]
      \mu_{t} &= \mu_{t - 1} + \omega_{t} & \omega_{t} & \sim \dnorm{0, \tau^{2} \lambda^{2}_{t}} \\
      \lambda_{t} &\sim \dhalfcauchy{0, \eta_{t}} \\
      \eta_{t} &\sim \dhalfcauchy{0, 1} \\
      \tau &\sim \dhalfcauchy{0, \frac{1}{n}}
    \end{aligned}
  \end{equation}
\end{description}


\begin{figure}[htpb!]
  \centering
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[]{\Sexpr{file.path(fig_path, "nile-m0_mu-1.pdf")}}
    \caption{\ModelII{Constant}}
  \end{subfigure}%
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[]{\Sexpr{file.path(fig_path, "nile-m1_mu-1.pdf")}}
    \caption{\ModelII{Intervention}}
  \end{subfigure}
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[]{\Sexpr{file.path(fig_path, "nile-m2_mu-1.pdf")}}    
    \caption{\ModelII{Normal}}
  \end{subfigure}%
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[]{\Sexpr{file.path(fig_path, "nile-m3_mu-1.pdf")}}
    \caption{\ModelII{StudentT}}
  \end{subfigure}
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[]{\Sexpr{file.path(fig_path, "nile-m4_mu-1.pdf")}}    
    \caption{\ModelII{Laplace}}
  \end{subfigure}%
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[]{\Sexpr{file.path(fig_path, "nile-m5_mu-1.pdf")}}
    \caption{\ModelII{Horseshoe}}
  \end{subfigure}
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[]{\Sexpr{file.path(fig_path, "nile-m6_mu-1.pdf")}}    
    \caption{\ModelII{Horseshoe+}}
  \end{subfigure}
  \caption{Posterior distributions of $\mu_t$ for each model. The line is the posterior mean; the range of the inner ribbon are the 16--84\% percentiles, the outer ribbon the 2.5--97.5\% percentiles.}
  \label{fig:nile_mu_posterior}
\end{figure}


\begin{figure}[htpb!]
  \centering
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[]{\Sexpr{file.path(fig_path, "nile-m2_omega-1.pdf")}}    
    \caption{\ModelII{Normal}}
  \end{subfigure}%
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[]{\Sexpr{file.path(fig_path, "nile-m3_omega-1.pdf")}}
    \caption{\ModelII{StudentT}}
  \end{subfigure}
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[]{\Sexpr{file.path(fig_path, "nile-m4_omega-1.pdf")}}    
    \caption{\ModelII{Laplace}}
  \end{subfigure}%
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[]{\Sexpr{file.path(fig_path, "nile-m5_omega-1.pdf")}}
    \caption{\ModelII{Horseshoe}}
  \end{subfigure}
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[]{\Sexpr{file.path(fig_path, "nile-m6_omega-1.pdf")}}    
    \caption{\ModelII{Horseshoe+}}
  \end{subfigure}
  \caption{Posterior distributions of $\omega_t$ for each model. The point is the posterior mean; the range of the line is the 2.5--97.5\% percentiles.}
  \label{fig:nile_omega_posterior}
\end{figure}



\begin{table}[thbp]
  \centering
  \input{\Sexpr{file.path(tab_path, "nile-tab_model_comp.tex")}}
  \caption{
    Root mean squared errors (RMSE) of each model, $RMSE = \frac{1}{n} \sum_{i = 1}^{n} {\left( y_{i} - \E(\mu) \right)}^{2}$.
    $\mathrm{elpd}_{WAIC}$ and $\mathrm{elpd}_{loo}$ expected log predictive density implied by the Widely Applicable Information Criteria (WAIC) and approximate Leave One Out (LOO).
  }
\end{table}


\section{Change Points in Levels and Trends}
\label{sec:linear-filtering}


The use of sparsity inducing priors can be extended to model change points in trends in addition to the level.
\textcite{KimKohBoydEtAl2009} and \textcite{Tibshirani2014} consider trend filtering in a maximum likelihood framework with $\ell_{1}$ regularization.
This can be extended to Bayesian approaches using GDLMs and sparsity inducing shrinkage priors.
While the change point model in the 

\begin{equation}
  \label{eq:20}
  \begin{aligned}[t]
    y_{t} &= \mu_{t} + \epsilon_{t} & \epsilon_{t} & \sim \dnorm{0, \sigma^{2}} \\
    \mu_{t} &= \mu_{t - 1} + \partial \mu_{t - 1} + \omega_{1, t} \\
    \partial \mu_{t} &= + \partial \mu_{t - 1} + \omega_{2, t} \\
  \end{aligned}
\end{equation}


\begin{equation}
\label{eq:26}
\begin{bmatrix}
  \omega_{1, t} \\
  \omega_{2, t}
\end{bmatrix}
\sim \dnorm{0,
  \begin{bmatrix}
    \tau_{1}^{2} \lambda_{1, t}^{2} + \tau_{2} \lambda_{2, t}^{2} & \tau_{2} \lambda_{2, t} \\
    \tau_{2}^{2} \lambda_{2, t} & \tau_{2}^{2} \lambda_{2, t}
  \end{bmatrix}
}
\end{equation}


\section{Example: George W. Bush Approval Ratings}
\label{sec:george-w.-bush}

\textcite{RatkovicEng2010} use the approval ratings for George W. Bush, displayed in Figure \ref{fig:bush_approval}.
George W. Bush's approval ratings are difficult to fit with a typical smoothing methods because it was subject to two large jumps in his approval ratings: September 11th, 2001 and at the start of the Iraq War on March 20, 2003.
The data used in this example consist of \Sexpr{nrow(BushApproval)} polls between \Sexpr{date_format(min(BushApproval[["start_date"]]))} and \Sexpr{date_format(min(BushApproval[["start_date"]]))} from the Roper Center Public Opinion \footnote{From \url{http://webapps.ropercenter.uconn.edu/CFIDE/roper/presidential/webroot/presidential_rating_detail.cfm?allRate=True\&presidentName=Bush\#.UbeB8HUbyv8}.}

\begin{equation}
  \label{eq:20}
  \begin{aligned}[t]
    y_{t} &= \mu_{t} + \epsilon_{t} & \epsilon_{t} & \sim \dnorm{0, \sigma^{2}} \\
    \mu_{t} &= \alpha_{t} +  \mu_{t - 1} + \partial \mu_{t - 1} + \omega_{1, t} \\
    \partial \mu_{t} &= + \partial \mu_{t - 1} + \omega_{2, t} \\
    \begin{bmatrix}
      \omega_{1, t} \\
      \omega_{2, t}
    \end{bmatrix} &
                    \sim \dnorm{0,
                    \begin{bmatrix}
                      \tau_{1}^{2} \lambda_{1, t}^{2} + \tau_{2} \lambda_{2, t}^{2} & \tau_{2} \lambda_{2, t} \\
                      \tau_{2}^{2} \lambda_{2, t} & \tau_{2}^{2} \lambda_{2, t}
                    \end{bmatrix}
                    }
  \end{aligned}
\end{equation}


\begin{description}[font = \normalfont\ModelII]
\item[Normal]
  System errors are distributed normal.
  $\lambda_{i, t} = 1$  for all $i$ for all $t$, $\alpha_{t} = 0$ for all $t$.
\item[Intervention]
  System errors are distributed normal, $\lambda_{i, t} = 1$  for all $i$ for all $t$.
  There are manual interventions after 9/11 and the the Iraq War. 
  The values of $\alpha$ for those two dates are non-zero and estimated, all other $\alpha_{t}$ are zero.
  This corresponds to a manual intervention for known change points.
\item[Horseshoe] The system errors are distributed horseshoe. $\alpha_{t} = 0$ for all $t$, and 
  \begin{equation}
    \label{eq:27}
    \begin{aligned}[t]
    \lambda_{i, t} & \sim \dhalfcauchy{0, 1} & \text{for $i \in 1, 2$} \\
    \tau_{i} & \sim \dcauchy{0, \frac{1}{n}}
    \end{aligned}
  \end{equation}
\item[Horseshoe+] The system errors are distributed horseshoe+ $\alpha_{t}= 0$ for all $t$, and
  \begin{equation}
    \label{eq:25}
    \begin{aligned}
    \lambda_{i, t} & \sim \dhalfcauchy{0, \eta_{i, t}} &&\text{  for $i \in 1, 2$.} \\
    \eta_{i, t} & \sim \dhalfcauchy{0, 1} \\
    \tau_{i} & \sim \dcauchy{0, \frac{1}{n}}
    \end{aligned}
  \end{equation}
\end{description}


\begin{figure}[thbp!]
  \centering
  \includegraphics{\Sexpr{file.path(fig_path, "bush-approval-1")}}
  \caption{Approval ratings of President George W. Bush}
  \label{fig:bush_approval}
\end{figure}

\begin{figure}[thbp!]
  \centering
  \begin{subfigure}[b]{\linewidth}
    \includegraphics[]{\Sexpr{file.path(fig_path, "bush-m1_mu-1.pdf")}}
    \caption{\ModelII{Normal}}
  \end{subfigure}

  \begin{subfigure}[b]{\linewidth}
    \includegraphics[]{\Sexpr{file.path(fig_path, "bush-m3_mu-1.pdf")}}
    \caption{\ModelII{Intervention}}
  \end{subfigure}
  \caption{Posterior distribution of $\mu$ for \ModelII{Normal} and \ModelII{Intervention} models.}
  \label{fig:bush_mu1}
\end{figure}

\begin{figure}[thbp!]
  \begin{subfigure}[b]{\linewidth}
    \includegraphics[]{\Sexpr{file.path(fig_path, "bush-m2_mu-1.pdf")}}
    \caption{\ModelII{Horseshoe}}
  \end{subfigure}

  \begin{subfigure}[b]{\linewidth}
    \includegraphics[]{\Sexpr{file.path(fig_path, "bush-m4_mu-1.pdf")}}
    \caption{\ModelII{Horseshoe+}}
  \end{subfigure}
  \caption{Posterior distribution of $\mu$ for \ModelII{Horseshoe} and \ModelII{Horseshoe+} models.}
  \label{fig:bush_mu2}
\end{figure}


Model comparison

\begin{table}[thbp!]
  \centering
  \input{\Sexpr{file.path(tab_path, "bush-tab_model_comp.tex")}}
  \caption{}
  \label{tab:bush_model_comp}
\end{table}



\section{Discussion}
\label{sec:discussion}




\section{Example Stan Program}
\label{sec:example-stan-program}

Example of the code to estimate the Horseshoe change point model.
The DLM related user-defined functions in the \texttt{functions} block are excluded.


\inputminted[firstline=5,style=bw]{stan}{\Sexpr{file.path(PROJECT_ROOT, "stan", "changepoint_horseshoe.stan.mustache")}}  


%  LocalWords:  iid Efron GiordaniKohn2008 Tibshirani1996 Tipping2001
%  LocalWords:  PolsonScott2010 ParkCasella2008 Hans2009 DLMs Kalman
%  LocalWords:  CaronDoucet2008 BrownGriffin2010 FFBS Tusell2011 HMC
%  LocalWords:  CarvalhoPolsonScott2010 DurbinKoopmans2012 TVP Polson
%  LocalWords:  ReisSalazarGamerman2006 Ratkovic Carvalho SDDLM XXXX
%  LocalWords:  ARIMA GDLM GDLMs CalderiaZorn1998 WesternKleykamp2004
%  LocalWords:  Spirling2007a Spirling2007b Park2010 Park2011 OECD
%  LocalWords:  Blackwell2012 MitchellBeauchamp1988a Efron2008a rstan
%  LocalWords:  Graybacks Page1954a Hinkley1970a BaiPerron2003a XXX
%  LocalWords:  OlshenVenkatramanLucitoEtAl2004 BaiPerron1998 Yao1984
%  LocalWords:  KillickFearnheadEckley2012 BarryHartigan1993 Chib1998
%  LocalWords:  Fearnhead2006a FearnheadLiu2007a TibshiraniEtAl2005
%  LocalWords:  HarchaouiLevy Leduc2010 ChanYauZhang2014 Balke1993
%  LocalWords:  BhadraDattaPolsonEtAl2015a CarvalhoPolsonScott2009
%  LocalWords:  DattaGhosh2012 PasKleijnVaart2014a DeJongPenzer1998
%  LocalWords:  PetrisPetroneEtAl2009 DurbinKoopman2012 Cobb1978 RMSE
%  LocalWords:  CommandeurKoopmanOoms2011 StudentT JuarezSteel2010b
%  LocalWords:  WestHarrison1997 ShumwayStoffer2010 SARIMA Harvey1989
%  LocalWords:  CarterKohn1994 Fruehwirth Schnatter1994 paremeters
%  LocalWords:  DeJongShephard1995 DurbinKoopman2002 Stan2015a WAIC
%  LocalWords:  CarpenterGelmanHoffmanEtAl2015a HoffmanGelman2014a
%  LocalWords:  VanDykPark2008a LOO Tibshirani2014 KimKohBoydEtAl2009
%  LocalWords:  RatkovicEng2010 nrow BushApproval DLM stan
